{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.ipynb\n",
    "%run ./nlp.ipynb\n",
    "%run ./relation_extraction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text, relations = extract_relations(\n",
    "    \"parsed/race/train/middle/2549.txt.tree\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = relations[\"Explanation\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ExtendedRelationInfo at 0x10b809128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_extended_info(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_subjects_differ(text, rel, n_prev_sent=5):\n",
    "    new_st, left_segment_sent_no = move_st(text, rel.left.start, n_prev_sent)\n",
    "    if len({\".\", \"!\", \"?\"}.intersection(\n",
    "        set(word_tokenize(text[rel.left.start:rel.right.end])))) > 0:\n",
    "        right_segment_sent_no = left_segment_sent_no + 1\n",
    "    else:\n",
    "        right_segment_sent_no = left_segment_sent_no\n",
    "\n",
    "    if left_segment_sent_no == right_segment_sent_no:\n",
    "        return True\n",
    "    else:\n",
    "        resolved_subjects = get_resolved_subjects(text[new_st:rel.right.end])\n",
    "        if len(resolved_subjects) < right_segment_sent_no:\n",
    "            return True\n",
    "        else:\n",
    "            return (\n",
    "                resolved_subjects[left_segment_sent_no] \n",
    "                != resolved_subjects[right_segment_sent_no]\n",
    "            )\n",
    "\n",
    "        \n",
    "def test__do_subjects_differ():\n",
    "    text = \"\"\"Nika lives in Berlin. She goes to a Kita. Her favourite color is yellow.\"\"\"\n",
    "    assert do_subjects_differ(\n",
    "        text, \n",
    "        Relation(\"\", Segment(\"N\", 22, 42), Segment(\"S\", 42, len(text)), None, None)\n",
    "    )\n",
    "    text = \"\"\"Nika lives in Berlin. She goes to a Kita. She likes yellow things.\"\"\"\n",
    "    assert not do_subjects_differ(\n",
    "        text, \n",
    "        Relation(\"\", Segment(\"N\", 22, 42), Segment(\"S\", 42, len(text)), None, None)\n",
    "    )    \n",
    "\n",
    "    \n",
    "test__do_subjects_differ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_access(tokens, i):\n",
    "    if i < len(tokens):\n",
    "        return tokens[i]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItMakesParseResult = collections.namedtuple(\n",
    "    \"ItMakesParseResult\", [\"success\", \"subject\", \"verb\", \"rest\"]\n",
    ")\n",
    "\n",
    "\n",
    "def safe_access(tokens, i):\n",
    "    if i < len(tokens):\n",
    "        return tokens[i]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "def normalise_and_identify_person(subject):\n",
    "    subject = subject.lower()\n",
    "    if subject == \"me\":\n",
    "        return \"I\", 1\n",
    "    elif subject == \"us\":\n",
    "        return \"we\", 1\n",
    "    elif subject == \"you\":\n",
    "        return \"you\", 2\n",
    "    elif subject == \"him\":\n",
    "        return \"he\", 3\n",
    "    elif subject == \"her\":\n",
    "        return \"she\", 3\n",
    "    elif subject == \"them\":\n",
    "        return \"they\", 3\n",
    "    else:\n",
    "        return subject, 3\n",
    "    \n",
    "    \n",
    "def parse_it_makes(tokens, verbose=False):\n",
    "    if safe_access(tokens, 0).lower() == \"it\":\n",
    "        make_pos = -1\n",
    "        \n",
    "        if normalise_verb(safe_access(tokens, 1).lower()) == \"make\":\n",
    "            make_pos = 1\n",
    "        elif normalise_verb(safe_access(tokens, 2).lower()) == \"make\":\n",
    "            make_pos = 2\n",
    "        if make_pos < 0:\n",
    "            print_if_verbose(\"Didn't find 'make'.\", verbose)\n",
    "            return ItMakesParseResult(False, None, None, None)\n",
    "        else:\n",
    "            first_verb_position = find_first_verb(\" \".join(tokens[make_pos + 1:]))\n",
    "            if first_verb_position is None:\n",
    "                print_if_verbose(\"Didn't find the verb. Will look for an adjective.\", verbose)\n",
    "                first_adj_position = find_first_adjective(\" \".join(tokens[make_pos + 1:]))\n",
    "                if first_adj_position is None:\n",
    "                    print_if_verbose(\"Didn't find an adjective either.\")\n",
    "                    return ItMakesParseResult(False, None, None, None)\n",
    "                else:\n",
    "                    verb = \"be\"\n",
    "                    subject = \" \".join(\n",
    "                        tokens[make_pos + 1:(make_pos + 1 + first_adj_position)]\n",
    "                    )\n",
    "                    rest = \" \".join(tokens[(make_pos + 1 + first_adj_position):])\n",
    "            else:\n",
    "                verb = tokens[make_pos + 1 + first_verb_position]\n",
    "                subject = \" \".join(\n",
    "                    tokens[make_pos + 1:(make_pos + 1 + first_verb_position)]\n",
    "                )\n",
    "                rest = \" \".join(tokens[(make_pos + 1 + first_verb_position + 1):])\n",
    "            \n",
    "            if tokens[make_pos] in {\"make\", \"makes\", \"making\"}:\n",
    "                tense = Tense.PRESENT\n",
    "            elif tokens[make_pos] == \"made\":\n",
    "                tense = Tense.PAST\n",
    "            else:\n",
    "                assert False\n",
    "            normalised_subject, person = normalise_and_identify_person(subject)\n",
    "            if is_plural(normalised_subject):\n",
    "                number = Number.PLURAL\n",
    "            else:\n",
    "                number = Number.SINGULAR\n",
    "            return ItMakesParseResult(\n",
    "                True, \n",
    "                normalised_subject, \n",
    "                conjugate(verb, tense, person, number),\n",
    "                rest\n",
    "            )\n",
    "    else:\n",
    "        return ItMakesParseResult(False, None, None, None)\n",
    "\n",
    "        \n",
    "def check_parse_result(parse_result, true_subject, true_verb):\n",
    "    return parse_result.subject == true_subject and parse_result.verb == true_verb\n",
    "\n",
    "\n",
    "def test__parse_it_makes():\n",
    "    parse_result = parse_it_makes(tokenize(\"It's making him feel happy.\"))\n",
    "    check_parse_result(parse_result, \"he\", \"feels\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It makes them feel happy.\"))\n",
    "    check_parse_result(parse_result, \"they\", \"feel\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It makes the cats happy.\"))\n",
    "    check_parse_result(parse_result, \"the cats\", \"are\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It makes the cat happy.\"))\n",
    "    check_parse_result(parse_result, \"the cat\", \"is\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It has made the cat happy.\"))\n",
    "    check_parse_result(parse_result, \"the cat\", \"was\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It has made the cats happy.\"))\n",
    "    check_parse_result(parse_result, \"the cat\", \"were\")\n",
    "\n",
    "    parse_result = parse_it_makes(tokenize(\"It has made the cats go home.\"))\n",
    "    check_parse_result(parse_result, \"the cats\", \"went\")\n",
    "    \n",
    "    parse_result = parse_it_makes(tokenize(\"It has made Mr. Smith go home.\"))\n",
    "    check_parse_result(parse_result, \"Mr. Smith\", \"went\")\n",
    "\n",
    "test__parse_it_makes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rule_explanation_01(text, relation, verbose=False):\n",
    "    assert(relation is not None and relation.type == \"Explanation\")\n",
    "    info = prepare_extended_info(relation, verbose)\n",
    "    if info is None:\n",
    "        print_if_verbose(\"Extended info preparation wasn't successful.\", verbose)\n",
    "        return None, None\n",
    "\n",
    "    nucleus_tokens = tokenize(\n",
    "        info.nucleus_text\n",
    "    )\n",
    "    assert len(nucleus_tokens) > 0\n",
    "    print_if_verbose(f\"Satellite's nucleus text: \\n{info.sn_text}\", verbose)\n",
    "    sn_tokens = tokenize(\n",
    "        info.sn_text\n",
    "    )\n",
    "    assert len(sn_tokens) > 0\n",
    "    \n",
    "    condition_holds = False\n",
    "    augmented_sn_text = info.sn_text\n",
    "    \n",
    "    if nucleus_tokens[0] == \"but\":\n",
    "        print_if_verbose(\"Nucleus starts with 'but'.\", verbose)\n",
    "        condition_holds = True\n",
    "    elif (\n",
    "        nucleus_tokens[0] == \"if\"\n",
    "            and (\n",
    "                info.nucleus_info.relation is None\n",
    "                    or get_relation_type(info.nucleus_info.relation.left_child) != \"Condition\"\n",
    "            )\n",
    "    ):\n",
    "        print_if_verbose(\n",
    "            \"Nucleus starts with 'if' \"\n",
    "            \"and its left subrelation is not 'Condition'.\", \n",
    "            verbose\n",
    "        )\n",
    "        condition_holds = True        \n",
    "    elif \"because\" in nucleus_tokens:\n",
    "        print_if_verbose(\"Nucleus contains 'because'.\", verbose)\n",
    "        condition_holds = True\n",
    "    elif sn_tokens[0] == 'but':\n",
    "        print_if_verbose(\"Satellite's nucleus starts with 'but'.\", verbose)\n",
    "        condition_holds = True\n",
    "    elif \"because\" in set(sn_tokens):\n",
    "        print_if_verbose(\"Satellite's nucleus contains 'because'.\", verbose)\n",
    "        condition_holds = True\n",
    "    else:\n",
    "        satellite_text = text[\n",
    "            info.satellite_info.segment.start:info.satellite_info.segment.end\n",
    "        ]\n",
    "        print_if_verbose(f\"Satellite:\\n{satellite_text}\", verbose)\n",
    "        it_make_parse_result = parse_it_makes(word_tokenize(satellite_text))\n",
    "        print_if_verbose(it_make_parse_result, verbose)\n",
    "        if it_make_parse_result.success:\n",
    "            print_if_verbose(\"Satellite's nucleus starts with 'It makes/made'.\", verbose)\n",
    "            condition_holds = True\n",
    "            assert it_make_parse_result.subject is not None\n",
    "            assert it_make_parse_result.verb is not None\n",
    "            assert it_make_parse_result.rest is not None\n",
    "            augmented_sn_text = clean(\n",
    "                    \" \".join(\n",
    "                    [\n",
    "                        it_make_parse_result.subject, \n",
    "                        it_make_parse_result.verb, \n",
    "                        it_make_parse_result.rest\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if condition_holds:\n",
    "        processed_sn_text = remove_leading_words(augmented_sn_text, verbose)\n",
    "        statement = \"{nucleus} because {satellite_nucleus}\".format(\n",
    "            nucleus=remove_trailing_punctuation(\n",
    "                uppercase_first_letter(info.nucleus_text)\n",
    "            ),\n",
    "            satellite_nucleus=lowercase_first_letter(\n",
    "                processed_sn_text if processed_sn_text is not None \n",
    "                    else info.sn_text\n",
    "            )\n",
    "        )\n",
    "        return statement, info.nucleus_proximity\n",
    "    else:\n",
    "        print_if_verbose(\"None of the conditions were met.\", verbose)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"parsed/race/train/middle/8123.txt.tree\", \"rt\") as f:\n",
    "    tree_text = f.read()\n",
    "\n",
    "text, relations = read_relations(tree_text.replace(\"<s>\", \"\").replace(\"<P>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I 'm often there for three hours .  It makes me feel great to do something for the environment .  \""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl = relations[\"Explanation\"][1]\n",
    "text[expl.left.start:expl.right.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleus is on the left.\n",
      "Satellite's nucleus is on the right.\n",
      "Nuclei proximity is far\n",
      "Satellite doesn't have nested relations or its depth is too small.\n",
      "Satellite doesn't contain a wh-word or 'how'.\n",
      "Satellite's nucleus text: \n",
      "me feel great to do something for the environment.\n",
      "Satellite:\n",
      "It makes me feel great to do something for the environment .  \n",
      "ItMakesParseResult(success=True, subject='I', verb='feel', rest='great to do something for the environment .')\n",
      "Satellite's nucleus starts with 'It makes/made'.\n",
      "Removed tokens before the first NP: \n",
      "I feel great to do something for the environment. \n",
      "---> \n",
      "I feel great to do something for the environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"I'm often there for three hours because I feel great to do something for the environment.\",\n",
       " 'far')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_explanation_01(text, expl, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"parsed/race/train/middle/276.txt.tree\", \"rt\") as f:\n",
    "    tree_text = f.read()\n",
    "\n",
    "text, relations = read_relations(tree_text.replace(\"<s>\", \"\").replace(\"<P>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Because there is n\\'t much oxygen there , trains will have oxygen masks for those who need _ .  It makes passengers feel more comfortable when they have enough oxygen on the famous \" roof of the world \" .  '"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl = relations[\"Explanation\"][1]\n",
    "text[expl.left.start:expl.right.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleus is on the left.\n",
      "Satellite's nucleus is on the right.\n",
      "Nuclei proximity is far\n",
      "Satellite doesn't have nested relations or its depth is too small.\n",
      "Satellite's nucleus contains a wh-word or 'how' in the middle and will be cut at its position.\n",
      "Satellite's nucleus text: \n",
      "passengers feel more comfortable.\n",
      "Nucleus contains 'because'.\n",
      "Removed tokens before the first NP: \n",
      "passengers feel more comfortable. \n",
      "---> \n",
      "passengers feel more comfortable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Because there is n't much oxygen there, trains will have oxygen masks for those who need _ because passengers feel more comfortable.\",\n",
       " 'far')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_explanation_01(text, expl, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"parsed/race/train/middle/2459.txt.tree\", \"rt\") as f:\n",
    "    tree_text = f.read()\n",
    "\n",
    "text, relations = read_relations(tree_text.replace(\"<s>\", \"\").replace(\"<P>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We helped him go through many medical examinations .  After all kinds of tests , we were told that the final diagnosis was appendicitis .  Luckily , Sam did n't need an operation because we brought him to the hospital in time .  He got timely treatment .  After Sam felt much better , we rushed back to prepare ourselves for the class .  We were a little tired , but we had certainly done something good , something right .  \""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl = relations[\"Explanation\"][2]\n",
    "text[expl.left.start:expl.right.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleus is on the left.\n",
      "Satellite's nucleus is on the left.\n",
      "Nuclei proximity is near\n",
      "Satellite doesn't have nested relations or its depth is too small.\n",
      "Satellite doesn't contain a wh-word or 'how'.\n",
      "Satellite's nucleus text: \n",
      "After all kinds of tests, we were told that the final diagnosis was appendicitis.  Luckily, Sam did n't need an operation because we brought him to the hospital in time.  He got timely treatment.\n",
      "Satellite's nucleus contains 'because'.\n",
      "Removed tokens before the first NP: \n",
      "After all kinds of tests, we were told that the final diagnosis was appendicitis.  Luckily, Sam did n't need an operation because we brought him to the hospital in time.  He got timely treatment. \n",
      "---> \n",
      "all kinds of tests, we were told that the final diagnosis was appendicitis. Luckily, Sam did n't need an operation because we brought him to the hospital in time. He got timely treatment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"We helped him go through many medical examinations because all kinds of tests, we were told that the final diagnosis was appendicitis. Luckily, Sam did n't need an operation because we brought him to the hospital in time. He got timely treatment.\",\n",
       " 'near')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_explanation_01(text, expl, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"statements/explanation_rule_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {os.path.join(OUTPUT_DIR, \"train\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"parsed/race/train/middle\"\n",
    "\n",
    "statements_near, statements_far = [], []\n",
    "for file_name in os.listdir(DIRECTORY):\n",
    "    path = os.path.join(DIRECTORY, file_name)\n",
    "    text, relations = extract_relations(path)\n",
    "    \n",
    "    if \"Explanation\" in relations:\n",
    "        for relation in relations[\"Explanation\"]:\n",
    "            statement, nucleus_proximity = rule_explanation_01(\n",
    "                text, \n",
    "                relation\n",
    "            )\n",
    "            if statement is not None:\n",
    "                statement_str = f\"[{path}]\\n{statement}\"\n",
    "                if nucleus_proximity == \"near\":\n",
    "                    statements_near.append(statement_str)\n",
    "                else:\n",
    "                    statements_far.append(statement_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(OUTPUT_DIR, \"train/middle_near.txt\"),  \n",
    "    \"wt\"\n",
    ") as f:\n",
    "    f.write(\"\\n\".join(statements_near))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(OUTPUT_DIR, \"train/middle_near.txt\"),  \n",
    "    \"wt\"\n",
    ") as f:\n",
    "    f.write(\"\\n\".join(statements_far))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parsed/race/train/middle/2458.txt.tree]\n",
      "I felt a little angry because Swift is my favourite star because i could n't understand.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/2459.txt.tree]\n",
      "We helped him go through many medical examinations because all kinds of tests, we were told that the final diagnosis was appendicitis. Luckily, Sam did n't need an operation because we brought him to the hospital in time. He got timely treatment.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/2459.txt.tree]\n",
      "I really wanted to go on sleeping, as I had insomnia and had just fallen asleep, but I could n't because Sam groaned louder and louder, showing he was seriously ill because that time, Robbie, a student from China, also got up to help.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/4750.txt.tree]\n",
      "But he is going to have English and Chinese lessons because he thinks hard and finds a way.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/1358.txt.tree]\n",
      "Because he drank too much, he was drunk and fell to the ground because the rich man came back, he could n't find his food and his wine.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/1358.txt.tree]\n",
      "After the rich man was away from his home, he enjoyed a nice meal because he drank too much, he was drunk and fell to the ground.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/5403.txt.tree]\n",
      "\" I got the job because the people who run the firm knew I had already written some programs, \" he said because I suppose $ 150,000 sounds a lot but I hope it will come to more than that this year. '' He spends some of his money on records and clothes, and gives his mother $ 20 a week as he lives with his parents.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/6165.txt.tree]\n",
      "If you do n't, you may get lost because you really get lost, this is.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/6165.txt.tree]\n",
      "If you get into the forest with your friends, stay with them always because you do n't, you may get lost.\n",
      "\n",
      "\n",
      "[parsed/race/train/middle/7158.txt.tree]\n",
      "But only the rich were able to leave because we go?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in statements_near[:10]:\n",
    "    print(s)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
