{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from collections import namedtuple\n",
    "\n",
    "import pattern.en as en\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Constituency = namedtuple(\"Constituency\", [\"tag\", \"startIndex\", \"endIndex\", \"depth\"])\n",
    "\n",
    "\n",
    "def skip_blank_space(parse, pointer):\n",
    "    while pointer < len(parse) and parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return pointer\n",
    "\n",
    "\n",
    "def read_tag(parse, pointer):\n",
    "    tag_start = pointer\n",
    "    while pointer < len(parse) and not parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return parse[tag_start:pointer], skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_token(parse, pointer, tokens):\n",
    "    token_start = pointer\n",
    "    while pointer < len(parse) and parse[pointer] != \")\":\n",
    "        pointer += 1\n",
    "    tokens.append(parse[token_start:pointer])\n",
    "    return pointer\n",
    "    \n",
    "    \n",
    "def read_body(parse, pointer, constituencies, tokens, depth):\n",
    "    if parse[pointer] == \"(\":\n",
    "        return read_constituency(parse, pointer, constituencies, tokens, depth)\n",
    "    else:\n",
    "        return None, read_token(parse, pointer, tokens)\n",
    "        \n",
    "        \n",
    "def read_constituency(parse, pointer, constituencies, tokens, depth):\n",
    "    assert parse[pointer] == \"(\"\n",
    "    pointer += 1\n",
    "    tag, pointer = read_tag(parse, pointer)\n",
    "    first_child, pointer = read_body(parse, pointer, constituencies, tokens, depth + 1)\n",
    "    if first_child is None:\n",
    "        constituency = Constituency(\n",
    "            tag=tag, startIndex=len(tokens), endIndex=len(tokens) + 1, depth=depth\n",
    "        )\n",
    "        assert parse[pointer] == \")\"\n",
    "    else:\n",
    "        child = first_child\n",
    "        while parse[pointer] != \")\":\n",
    "            child, pointer = read_body(parse, pointer, constituencies, tokens, depth + 1)\n",
    "        constituency = Constituency(\n",
    "            tag=tag, startIndex=first_child.startIndex, endIndex=child.endIndex, depth=depth\n",
    "        )\n",
    "    pointer += 1\n",
    "    constituencies.append(constituency)  \n",
    "    return constituency, skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_constituencies(parse):\n",
    "    constituencies = []\n",
    "    tokens = []\n",
    "    read_constituency(parse, 0, constituencies, tokens, 0)\n",
    "    return constituencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject(parse):\n",
    "    constituencies = read_constituencies(parse)\n",
    "    np_constituencies = [c for c in constituencies if c.tag == \"NP\"]\n",
    "    if len(np_constituencies) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        top_np_constituency = sorted(np_constituencies, key=lambda c: c.depth)[0]\n",
    "        return top_np_constituency.startIndex, top_np_constituency.endIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_resolution_map(corefs):\n",
    "    resolution_map = {}\n",
    "\n",
    "    for _, coref_group in corefs.items():\n",
    "        representative_mention = None\n",
    "        keys = []\n",
    "        for coref in coref_group:\n",
    "            if coref[\"isRepresentativeMention\"]:\n",
    "                representative_mention = coref[\"text\"]\n",
    "            keys.append((coref[\"sentNum\"], coref[\"startIndex\"], coref[\"endIndex\"]))\n",
    "        for key in keys:\n",
    "            resolution_map[key] = representative_mention    \n",
    "            \n",
    "    return resolution_map\n",
    "\n",
    "\n",
    "def resolve_subject(sentence_num, subject, resolution_map, tokens):\n",
    "    key = (sentence_num, *subject)\n",
    "    if key in resolution_map:\n",
    "        return resolution_map[key]\n",
    "    else:\n",
    "        return \" \".join(\n",
    "            [tokens[index - 1][\"originalText\"] for index in range(*subject)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resolved_subjects(text, nlp=nlp):\n",
    "    result = nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse,coref',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    resolution_map = create_resolution_map(result[\"corefs\"])\n",
    "    \n",
    "    resolved_subjects = []\n",
    "    for i, sent in enumerate(result[\"sentences\"]):\n",
    "        subject = get_subject(sent[\"parse\"])\n",
    "        if subject is None:\n",
    "            resolved_subject = None\n",
    "        else:\n",
    "            resolved_subject = resolve_subject(\n",
    "                i + 1,\n",
    "                subject,\n",
    "                resolution_map,\n",
    "                result[\"sentences\"][i][\"tokens\"]\n",
    "            )\n",
    "        resolved_subjects.append(resolved_subject)\n",
    "    \n",
    "    return resolved_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test__get_resolved_subjects():\n",
    "    text = \"This movie was actually neither that funny, nor super witty. The movie was meh. I liked watching that movie. If I had a choice, I would not watch that movie again.\"\n",
    "    assert get_resolved_subjects(text, nlp) == [\"This movie\", \"This movie\", \"I\", \"I\"]\n",
    "    \n",
    "\n",
    "test__get_resolved_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_st(text, st, n_prev_sentences):\n",
    "    sent_counter = 0\n",
    "    while st > 0:\n",
    "        if text[st] in {\".\", \"!\", \"?\"}:\n",
    "            sent_counter += 1\n",
    "            if sent_counter > n_prev_sentences:\n",
    "                return st + 1, sent_counter - 1\n",
    "        st -= 1\n",
    "    return 0, sent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_tokens(sent, offset=1):\n",
    "    output_tokens = []\n",
    "    for token in [t[\"word\"] for t in sent[\"tokens\"][(offset - 1):]]:\n",
    "        if token not in {\".\", \",\", \"?\", \"!\", \";\", \":\"}:\n",
    "            output_tokens.append(\" \")\n",
    "        output_tokens.append(token)\n",
    "    if len(output_tokens) > 0 and output_tokens[0] == \" \":\n",
    "        output_tokens = output_tokens[1:]\n",
    "    return \"\".join(output_tokens)           \n",
    "\n",
    "\n",
    "def remove_leading_words(text, verbose=False):\n",
    "    result = nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if len(result[\"sentences\"]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        sent = result[\"sentences\"][0]\n",
    "        constituencies = read_constituencies(sent[\"parse\"])\n",
    "        np_constituencies = [c for c in constituencies if c.tag == \"NP\"]\n",
    "        if len(np_constituencies) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            first_np_constituency = sorted(np_constituencies, key=lambda c: c.startIndex)[0]\n",
    "            new_text = \" \".join(\n",
    "                [join_tokens(sent, first_np_constituency.startIndex)]\n",
    "                + [join_tokens(s) for s in result[\"sentences\"][1:]]\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Removed tokens before the first NP: \\n\"\n",
    "                    f\"{text} \\n---> \\n{new_text}\"\n",
    "                )\n",
    "            return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_verb(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def is_plural(word):\n",
    "    if word == en.singularize(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "class Tense(enum.Enum):\n",
    "    PRESENT = 0\n",
    "    PAST = 1\n",
    "\n",
    "    \n",
    "class Number(enum.Enum):\n",
    "    SINGULAR = 0\n",
    "    PLURAL = 1\n",
    "    \n",
    "    \n",
    "def conjugate(verb, tense=Tense.PAST, person=1, number=Number.SINGULAR):\n",
    "    if tense == Tense.PRESENT:\n",
    "        en_tense = en.PRESENT\n",
    "    elif tense == Tense.PAST:\n",
    "        en_tense = en.PAST\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    if number == Number.SINGULAR:\n",
    "        en_number = en.SINGULAR\n",
    "    elif number == Number.PLURAL:\n",
    "        en_number = en.PLURAL\n",
    "    else: \n",
    "        assert False\n",
    "        \n",
    "    return en.conjugate(verb, tense=en_tense, person=person, number=en_number)\n",
    "\n",
    "\n",
    "def find_first_verb(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag[:1] == 'V':\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_first_adjective(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag == 'JJ':\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
