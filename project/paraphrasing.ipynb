{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import nltk.tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pattern import en\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /Users/YK/mt/project/aux/relation_extraction.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/nlp.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import aux.relation_extraction\n",
    "import aux.nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATEMENTS_DIR = \"/Users/YK/mt/project/statements_5/\"\n",
    "RACE_PART = \"test/middle\"\n",
    "RACE_DIR = \"/Users/YK/mt/RACE\"\n",
    "PARSED_RACE_DIR = \"/Users/YK/mt/parsed/race\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Most Important Words (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_names = os.listdir(\n",
    "    os.path.join(\n",
    "        PARSED_RACE_DIR, RACE_PART\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(input_directory, file_name):\n",
    "    text, _, _ = aux.relation_extraction.load_relations(\n",
    "        os.path.join(input_directory, file_name)\n",
    "    )\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    read_file(\n",
    "        os.path.join(PARSED_RACE_DIR, RACE_PART), \n",
    "        file_name\n",
    "    )\n",
    "    for file_name in file_names[:10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    input=\"content\",\n",
    "    lowercase=False, \n",
    "    stop_words=\"english\", \n",
    "    norm=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.argsort(tfidf.todense(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(tfidf, tfidf_vectorizer, i, word):\n",
    "    return float(tfidf[i, tfidf_vectorizer.get_feature_names().index(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_important_words(tfidf_vectorizer, a, i):\n",
    "    return np.array(\n",
    "        tfidf_vectorizer.get_feature_names()\n",
    "    )[np.array(a[i, -10:]).flatten()[::-1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Easter', 'beach', 'chocolate', 'Australia', 'hunt', 'eggs',\n",
       "       'bilbies', 'animals', 'tents', 'camping'], dtype='<U13')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_words(tfidf_vectorizer, a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Swift', 'US', 'does', 'laughed', 'speak', 'understand', 'way',\n",
       "       'people', 'square', 'speech'], dtype='<U13')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_words(tfidf_vectorizer, a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonyms_df = pd.read_csv(\n",
    "    \"../filtered_edited_synonyms.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonyms_map = {\n",
    "    row[\"word\"]: row[\"synonyms\"].split(\", \") for _, row in synonyms_df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle_vocabulary_df = pd.read_csv(\"../middle_vocabulary.csv\", usecols=[0, 1])\n",
    "middle_vocabulary_df.columns=[\"word\", \"pos\"]\n",
    "middle_vocabulary = set([w.lower() for w in middle_vocabulary_df.word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_df = pd.read_csv(\"../words_to_exclude.csv\", header=None)\n",
    "\n",
    "words_to_exclude = []\n",
    "for i in range(4):\n",
    "    words_to_exclude.extend(\n",
    "        [line.split()[0].lower() for line in exclude_df.loc[~exclude_df[i].isnull(), i]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_past(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PAST,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def make_progressive(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        aspect = en.PROGRESSIVE,\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    ) \n",
    "\n",
    "\n",
    "def make_perfect(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PAST,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        aspect = en.PARTICIPLE,\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def make_third_person(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 3,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def is_past(verb):\n",
    "    return make_past(verb) == verb\n",
    "\n",
    "\n",
    "def is_progressive(verb):\n",
    "    return make_progressive(verb) == verb\n",
    "\n",
    "\n",
    "def is_perfect(verb):\n",
    "    return make_perfect(verb) == verb\n",
    "\n",
    "\n",
    "def is_third_person(verb):\n",
    "    return make_third_person(verb) == verb\n",
    "\n",
    "\n",
    "def is_verb(pos):\n",
    "    return pos[:1] == \"V\"\n",
    "\n",
    "\n",
    "def is_plural(word):\n",
    "    return en.singularize(word) != word\n",
    "\n",
    "\n",
    "def is_noun(pos):\n",
    "    return pos == \"NN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        tokens_with_space = [tokens[0]]\n",
    "        prev_word = tokens[0]\n",
    "        for token in tokens[1:]:\n",
    "            if (\n",
    "                token not in {\".\", \",\", \"!\", \"?\", \";\", \":\", \")\"} \n",
    "                    and token[:1] != \"'\"\n",
    "                    and prev_word != \"(\"\n",
    "            ):\n",
    "                tokens_with_space.append(\" \")\n",
    "            tokens_with_space.append(token)\n",
    "            prev_word = token\n",
    "        return aux.nlp.fix_quotes(\"\".join(tokens_with_space))\n",
    "    \n",
    "    \n",
    "def get_form(substitute, original_word, pos):\n",
    "    if is_verb(pos):\n",
    "        if is_past(original_word):\n",
    "            return make_past(substitute)\n",
    "        elif is_progressive(original_word):\n",
    "            return make_progressive(substitute)\n",
    "        elif is_perfect(original_word):\n",
    "            return make_perfect(substitute)\n",
    "        elif is_third_person(original_word):\n",
    "            return make_third_person(substitute)\n",
    "        else:\n",
    "            return substitute\n",
    "    elif is_noun(pos):\n",
    "        if is_plural(original_word):\n",
    "            return en.pluralize(substitute)\n",
    "        else:\n",
    "            return substitute\n",
    "    else:\n",
    "        return substitute\n",
    "    \n",
    "\n",
    "def is_in_vocabulary(synset, vocabulary):\n",
    "    return synset[0] in vocabulary\n",
    "\n",
    "\n",
    "def get_hypernym(token, pos, lemmatized_token, middle_vocabulary):\n",
    "    if is_verb(pos) or is_noun(pos) or pos == \"ADV\" or pos == \"ADJ\":\n",
    "        synsets = en.wordnet.synsets(lemmatized_token, pos=pos)\n",
    "        for synset in synsets:\n",
    "            if is_in_vocabulary(synset, middle_vocabulary):\n",
    "                for hypernym in synsets[0].hypernyms():\n",
    "                    if is_in_vocabulary(hypernym, middle_vocabulary):\n",
    "                        return get_form(hypernym[0], token, pos)\n",
    "    return None\n",
    "    \n",
    "\n",
    "def get_synonym(token, pos, lemmatized_token, synonyms_map):\n",
    "    if lemmatized_token in synonyms_map:\n",
    "        synonym = np.random.choice(synonyms_map[lemmatized_token], 1)[0]\n",
    "        return get_form(synonym, token, pos)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def paraphrase(\n",
    "    sentence, \n",
    "    synonyms_map, \n",
    "    middle_vocabulary, \n",
    "    synonym_stats, \n",
    "    hypernym_stats, \n",
    "    words_to_exclude,\n",
    "    use_hypernyms=True\n",
    "):\n",
    "    tokens, pos = zip(*en.tag(sentence))\n",
    "    tokens = list(tokens)\n",
    "    substituted = False\n",
    "    for i in range(len(tokens)):\n",
    "        lemmatized_token = en.lemma(tokens[i])\n",
    "        if not lemmatized_token in words_to_exclude:\n",
    "            synonym = get_synonym( # trying to substitute with a synonym first\n",
    "                tokens[i], pos[i], lemmatized_token, synonyms_map\n",
    "            )\n",
    "            if synonym is not None: \n",
    "                synonym_stats[lemmatized_token] += 1\n",
    "                tokens[i] = synonym\n",
    "                substituted = True\n",
    "            else: # no synonym found, trying to substitute with a hypernym\n",
    "                if use_hypernyms:\n",
    "                    hypernym = get_hypernym(\n",
    "                        tokens[i], pos[i], lemmatized_token, middle_vocabulary\n",
    "                    )\n",
    "                    if hypernym is not None:\n",
    "                        hypernym_stats[lemmatized_token] += 1\n",
    "                        tokens[i] = hypernym\n",
    "                        substituted = True\n",
    "    return join_tokens(tokens), substituted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He obtained rights to the record.', True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(\n",
    "    \"He gained access to the file.\", \n",
    "    synonyms_map, \n",
    "    middle_vocabulary,\n",
    "    collections.Counter(),\n",
    "    collections.Counter(),\n",
    "    words_to_exclude\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatives_df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"prefiltered_alternatives_test-middle_9bed806f.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paraphrase_statements(statements, synonyms_map, words_to_exclude, use_hypernyms=True):\n",
    "    paraphrased_statements = []\n",
    "    substitution_indicators = []\n",
    "    synonym_stats = collections.Counter()\n",
    "    hypernym_stats = collections.Counter()\n",
    "    for statement in statements:\n",
    "        paraphrased_statement, substituted = paraphrase(\n",
    "            statement, \n",
    "            synonyms_map, \n",
    "            middle_vocabulary, \n",
    "            synonym_stats, \n",
    "            hypernym_stats, \n",
    "            words_to_exclude,\n",
    "            use_hypernyms\n",
    "        )\n",
    "        paraphrased_statements.append(paraphrased_statement)\n",
    "        substitution_indicators.append(substituted)\n",
    "    return paraphrased_statements, substitution_indicators, synonym_stats, hypernym_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_paraphrased_statement_in_place(\n",
    "    df, column, synonyms_map, words_to_exclude, use_hypernyms=True\n",
    "):\n",
    "    paraphrased_statements, substitution_indicators, synonym_stats, hypernym_stats = \\\n",
    "        paraphrase_statements(\n",
    "            df[column], synonyms_map, words_to_exclude, use_hypernyms\n",
    "        )\n",
    "    prefix = \"synonym\" if not use_hypernyms else \"synonym_and_hypernym\"\n",
    "    df[f\"{prefix}_paraphrased_{column}\"] = paraphrased_statements\n",
    "    df[f\"was_{column}_{prefix}_paraphrased\"] = substitution_indicators\n",
    "    return synonym_stats, hypernym_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_paraphrased_statement_in_place(\n",
    "    alternatives_df, \"true_statement\", synonyms_map, words_to_exclude, False\n",
    ")\n",
    "add_paraphrased_statement_in_place(\n",
    "    alternatives_df, \"alternative_statement\", synonyms_map, words_to_exclude, False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonym_stats__true_statements, hypernym_stats__true_statements \\\n",
    "    = add_paraphrased_statement_in_place(\n",
    "        alternatives_df, \"true_statement\", synonyms_map, words_to_exclude\n",
    "    )\n",
    "synonym_stats__alternatives, hypernym_stats__alternatives = \\\n",
    "    add_paraphrased_statement_in_place(\n",
    "        alternatives_df, \"alternative_statement\", synonyms_map, words_to_exclude\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 65),\n",
       " ('all', 56),\n",
       " ('very', 54),\n",
       " ('good', 33),\n",
       " ('learn', 27),\n",
       " ('away', 27),\n",
       " ('big', 25),\n",
       " ('buy', 22),\n",
       " ('love', 20),\n",
       " ('father', 19),\n",
       " ('little', 19),\n",
       " ('new', 17),\n",
       " ('house', 16),\n",
       " ('job', 16),\n",
       " ('important', 15),\n",
       " ('idea', 15),\n",
       " ('kind', 14),\n",
       " ('great', 14),\n",
       " ('part', 14),\n",
       " ('earth', 13),\n",
       " ('story', 13),\n",
       " ('begin', 11),\n",
       " ('popular', 11),\n",
       " ('usually', 10),\n",
       " ('taxi', 10),\n",
       " ('writer', 10),\n",
       " ('place', 9),\n",
       " ('woman', 9),\n",
       " ('fly', 7),\n",
       " ('old', 7),\n",
       " ('easy', 7),\n",
       " ('decide', 7),\n",
       " ('problem', 7),\n",
       " ('hate', 7),\n",
       " ('receive', 6),\n",
       " ('quiet', 6),\n",
       " ('gift', 6),\n",
       " ('child', 5),\n",
       " ('round', 5),\n",
       " ('answer', 4),\n",
       " ('word', 4),\n",
       " ('beautiful', 4),\n",
       " ('happy', 4),\n",
       " ('cold', 3),\n",
       " ('explain', 3),\n",
       " ('arrive', 3),\n",
       " ('sure', 3),\n",
       " ('quite', 3),\n",
       " ('street', 3),\n",
       " ('strange', 3),\n",
       " ('fat', 3),\n",
       " ('wonderful', 2),\n",
       " ('sad', 2),\n",
       " ('right', 2),\n",
       " ('dark', 2),\n",
       " ('shop', 2),\n",
       " ('wrong', 2),\n",
       " ('nearly', 2),\n",
       " ('bad', 2),\n",
       " ('dear', 2),\n",
       " ('trouble', 2),\n",
       " ('kill', 2),\n",
       " ('angry', 2),\n",
       " ('trip', 2),\n",
       " ('raise', 2),\n",
       " ('trust', 1),\n",
       " ('cry', 1),\n",
       " ('crash', 1),\n",
       " ('fast', 1),\n",
       " ('brave', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_stats__true_statements.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fact', 180),\n",
       " ('time', 33),\n",
       " ('air', 18),\n",
       " ('area', 18),\n",
       " ('book', 17),\n",
       " ('man', 17),\n",
       " ('jump', 15),\n",
       " ('read', 14),\n",
       " ('tiger', 14),\n",
       " ('team', 13),\n",
       " ('boy', 13),\n",
       " ('walk', 13),\n",
       " ('number', 12),\n",
       " ('girl', 12),\n",
       " ('way', 12),\n",
       " ('banana', 12),\n",
       " ('adult', 12),\n",
       " ('party', 11),\n",
       " ('age', 10),\n",
       " ('mom', 10),\n",
       " ('friend', 9),\n",
       " ('seem', 9),\n",
       " ('example', 9),\n",
       " ('family', 9),\n",
       " ('dance', 9),\n",
       " ('math', 9),\n",
       " ('study', 8),\n",
       " ('letter', 8),\n",
       " ('information', 8),\n",
       " ('collect', 8),\n",
       " ('speed', 8),\n",
       " ('play', 8),\n",
       " ('exam', 7),\n",
       " ('colour', 7),\n",
       " ('sleep', 7),\n",
       " ('gentleman', 7),\n",
       " ('plastic', 6),\n",
       " ('library', 6),\n",
       " ('present', 6),\n",
       " ('birthday', 6),\n",
       " ('contain', 6),\n",
       " ('dinner', 6),\n",
       " ('factory', 6),\n",
       " ('repair', 6),\n",
       " ('c', 6),\n",
       " ('lunch', 6),\n",
       " ('drive', 6),\n",
       " ('entrance', 6),\n",
       " ('today', 6),\n",
       " ('phrase', 6),\n",
       " ('exercise', 6),\n",
       " ('housework', 6),\n",
       " ('winter', 6),\n",
       " ('consult', 6),\n",
       " ('category', 6),\n",
       " ('thing', 5),\n",
       " ('afternoon', 5),\n",
       " ('save', 5),\n",
       " ('send', 5),\n",
       " ('one', 5),\n",
       " ('power', 5),\n",
       " ('classroom', 4),\n",
       " ('produce', 4),\n",
       " ('room', 4),\n",
       " ('appear', 4),\n",
       " ('grandfather', 4),\n",
       " ('card', 4),\n",
       " ('history', 4),\n",
       " ('develop', 4),\n",
       " ('atmosphere', 4),\n",
       " ('tradition', 4),\n",
       " ('chance', 4),\n",
       " ('paid', 4),\n",
       " ('pair', 4),\n",
       " ('computer', 4),\n",
       " ('class', 4),\n",
       " ('mother', 4),\n",
       " ('born', 4),\n",
       " ('peace', 4),\n",
       " ('visit', 3),\n",
       " ('paper', 3),\n",
       " ('neighborhood', 3),\n",
       " ('swim', 3),\n",
       " ('finish', 3),\n",
       " ('kitchen', 3),\n",
       " ('post', 3),\n",
       " ('film', 3),\n",
       " ('baby', 3),\n",
       " ('bookshop', 3),\n",
       " ('backpack', 3),\n",
       " ('press', 3),\n",
       " ('sang', 3),\n",
       " ('sing', 3),\n",
       " ('public', 3),\n",
       " ('worker', 3),\n",
       " ('continue', 3),\n",
       " ('improve', 3),\n",
       " ('manage', 3),\n",
       " ('tonight', 3),\n",
       " ('hotel', 3),\n",
       " ('scientist', 3),\n",
       " ('shelf', 3),\n",
       " ('content', 3),\n",
       " ('difference', 3),\n",
       " ('piece', 2),\n",
       " ('game', 2),\n",
       " ('grandmother', 2),\n",
       " ('water', 2),\n",
       " ('dad', 2),\n",
       " ('outside', 2),\n",
       " ('bend', 2),\n",
       " ('fiber', 2),\n",
       " ('gang', 2),\n",
       " ('score', 2),\n",
       " ('goal', 2),\n",
       " ('table', 2),\n",
       " ('art', 2),\n",
       " ('magazine', 2),\n",
       " ('partner', 2),\n",
       " ('playground', 2),\n",
       " ('noon', 2),\n",
       " ('postage', 2),\n",
       " ('surprise', 2),\n",
       " ('daddy', 2),\n",
       " ('alarm', 2),\n",
       " ('wife', 2),\n",
       " ('speak', 2),\n",
       " ('language', 2),\n",
       " ('education', 2),\n",
       " ('prize', 2),\n",
       " ('seat', 2),\n",
       " ('invitation', 2),\n",
       " ('desk', 2),\n",
       " ('shout', 2),\n",
       " ('offer', 2),\n",
       " ('waste', 2),\n",
       " ('affect', 2),\n",
       " ('excuse', 2),\n",
       " ('realize', 2),\n",
       " ('corner', 2),\n",
       " ('style', 1),\n",
       " ('push', 1),\n",
       " ('pass', 1),\n",
       " ('restaurant', 1),\n",
       " ('chat', 1),\n",
       " ('ride', 1),\n",
       " ('bathroom', 1),\n",
       " ('inside', 1),\n",
       " ('discover', 1),\n",
       " ('stamp', 1),\n",
       " ('receiver', 1),\n",
       " ('supply', 1),\n",
       " ('conversation', 1),\n",
       " ('program', 1),\n",
       " ('holiday', 1),\n",
       " ('ship', 1),\n",
       " ('kite', 1),\n",
       " ('sail', 1),\n",
       " ('bridge', 1),\n",
       " ('newspaper', 1),\n",
       " ('uncertainty', 1),\n",
       " ('graduate', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym_stats__true_statements.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9323671497584541"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    alternatives_df.loc[:, \"was_true_statement_synonym_and_hypernym_paraphrased\"]\n",
    ") / len(alternatives_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_no', 'true_statement', 'nuclei_hash', 'alternative_statement',\n",
       "       'relation_type', 'position', 'distance_words', 'distance_sentences',\n",
       "       'sn_length', 'sn_length_relative_difference', 'jaccard_distance',\n",
       "       'edit_distance', 'rule', 'reason', 'synonym_paraphrased_true_statement',\n",
       "       'was_true_statement_synonym_paraphrased',\n",
       "       'synonym_paraphrased_alternative_statement',\n",
       "       'was_alternative_statement_synonym_paraphrased',\n",
       "       'synonym_and_hypernym_paraphrased_true_statement',\n",
       "       'was_true_statement_synonym_and_hypernym_paraphrased',\n",
       "       'synonym_and_hypernym_paraphrased_alternative_statement',\n",
       "       'was_alternative_statement_synonym_and_hypernym_paraphrased'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternatives_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df = alternatives_df[\n",
    "    [\n",
    "        \"text_no\",\n",
    "        \"true_statement\",\n",
    "        \"nuclei_hash\",\n",
    "        \"synonym_paraphrased_true_statement\",\n",
    "#         \"synonym_and_hypernym_paraphrased_true_statement\",\n",
    "        \"alternative_statement\",\n",
    "        \"synonym_paraphrased_alternative_statement\",\n",
    "#         \"synonym_and_hypernym_paraphrased_alternative_statement\",\n",
    "        \"was_true_statement_synonym_paraphrased\",\n",
    "        \"was_alternative_statement_synonym_paraphrased\",\n",
    "#         \"was_true_statement_synonym_and_hypernym_paraphrased\",\n",
    "#         \"was_alternative_statement_synonym_and_hypernym_paraphrased\"\n",
    "        \"reason\",\n",
    "        \"position\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The plastic leg Jeff can run. Moreover, Jeff made a plan with his friends who had plastic legs.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.iloc[0][\"true_statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The solid leg Jeff can run. Moreover, Jeff made a plan with his friends who had plastic legs.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.iloc[0][\"synonym_and_hypernym_paraphrased_true_statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.to_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"paraphrased_{RACE_PART.replace('/', '-')}_{random.randint(0, 2**32):x}.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    W2V_MODEL_DIR = \"../w2v_models\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            os.path.join(Word2VecModel.W2V_MODEL_DIR, f\"{model_name}.txt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_nolg_100d = Word2VecModel(\"enwiki_20180420_nolg_100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENTITY/Stour_and_Orwell_Walk', 0.732032835483551),\n",
       " ('path', 0.7227527499198914),\n",
       " ('führerweg', 0.7152807712554932),\n",
       " ('hopefully', 0.7044017314910889),\n",
       " ('ENTITY/WP:EL', 0.7037861347198486),\n",
       " ('ENTITY/WP:DR', 0.7026253938674927),\n",
       " ('jitler', 0.7024155855178833),\n",
       " ('cenzar', 0.7017824649810791),\n",
       " ('torpel', 0.7011029720306396),\n",
       " ('ENTITY/WP:Wikipedia', 0.7005131244659424)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('persons', 0.7992599010467529),\n",
       " ('residents', 0.7535831928253174),\n",
       " ('ENTITY/People', 0.7503395080566406),\n",
       " ('helsinkians', 0.7265914678573608),\n",
       " ('citizens', 0.7153019905090332),\n",
       " ('natives', 0.6979749798774719),\n",
       " ('youths', 0.6972818374633789),\n",
       " ('inhabitants', 0.694731593132019),\n",
       " ('tangbotens', 0.6896241903305054),\n",
       " ('americans', 0.6892567873001099)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENTITY/Chocolate', 0.9106836318969727),\n",
       " ('caramel', 0.8976747989654541),\n",
       " ('ENTITY/Caramel', 0.8792448043823242),\n",
       " ('ENTITY/White_chocolate', 0.8668451309204102),\n",
       " ('ENTITY/Peanut_butter', 0.8533239364624023),\n",
       " ('ENTITY/Types_of_chocolate', 0.8502905368804932),\n",
       " ('chocolates', 0.8490653038024902),\n",
       " ('ENTITY/Types_of_chocolate#Milk_chocolate', 0.8471919298171997),\n",
       " ('candy', 0.844744086265564),\n",
       " ('cream', 0.842796266078949)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"chocolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canceling', 0.8277071714401245),\n",
       " ('cancelling', 0.8080153465270996),\n",
       " ('postpone', 0.7998612523078918),\n",
       " ('reschedule', 0.7844353914260864),\n",
       " ('suspend', 0.7557451725006104),\n",
       " ('rescind', 0.7517332434654236),\n",
       " ('cancellation', 0.7479344010353088),\n",
       " ('cancels', 0.7384265661239624),\n",
       " ('announce', 0.7238617539405823),\n",
       " ('reconsider', 0.7106267213821411),\n",
       " ('renew', 0.7019257545471191),\n",
       " ('discontinue', 0.6998554468154907),\n",
       " ('postponing', 0.6976670026779175),\n",
       " ('postponement', 0.6904169917106628),\n",
       " ('ENTITY/Postpone_to_a_certain_time', 0.6869893670082092),\n",
       " ('announcing', 0.679280161857605),\n",
       " ('reinstate', 0.6781035661697388),\n",
       " ('revoke', 0.6780909299850464),\n",
       " ('rescheduling', 0.6767849922180176),\n",
       " ('renegotiate', 0.6732800602912903)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"cancel\", topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
