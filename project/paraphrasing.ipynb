{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import nltk.tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pattern import en\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import aux.relation_extraction\n",
    "import aux.nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATEMENTS_DIR = \"/Users/YK/mt/project/statements_3/\"\n",
    "RACE_PART = \"train/middle\"\n",
    "RACE_DIR = \"/Users/YK/mt/RACE\"\n",
    "PARSED_RACE_DIR = \"/Users/YK/mt/parsed/race\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Most Important Words (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_names = os.listdir(\n",
    "    os.path.join(\n",
    "        PARSED_RACE_DIR, RACE_PART\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(input_directory, file_name):\n",
    "    text, _, _ = aux.relation_extraction.load_relations(\n",
    "        os.path.join(input_directory, file_name)\n",
    "    )\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    read_file(\n",
    "        os.path.join(PARSED_RACE_DIR, RACE_PART), \n",
    "        file_name\n",
    "    )\n",
    "    for file_name in file_names[:10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    input=\"content\",\n",
    "    lowercase=False, \n",
    "    stop_words=\"english\", \n",
    "    norm=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.argsort(tfidf.todense(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(tfidf, tfidf_vectorizer, i, word):\n",
    "    return float(tfidf[i, tfidf_vectorizer.get_feature_names().index(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_important_words(tfidf_vectorizer, a, i):\n",
    "    return np.array(\n",
    "        tfidf_vectorizer.get_feature_names()\n",
    "    )[np.array(a[i, -10:]).flatten()[::-1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Easter', 'beach', 'chocolate', 'Australia', 'hunt', 'eggs',\n",
       "       'bilbies', 'animals', 'tents', 'camping'], dtype='<U13')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_words(tfidf_vectorizer, a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Swift', 'US', 'does', 'laughed', 'speak', 'understand', 'way',\n",
       "       'people', 'square', 'speech'], dtype='<U13')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_words(tfidf_vectorizer, a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_df = pd.read_csv(\n",
    "    \"../filtered_edited_synonyms.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_map = {\n",
    "    row[\"word\"]: row[\"synonyms\"].split(\", \") for _, row in synonyms_df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_vocabulary_df = pd.read_csv(\"../middle_vocabulary.csv\", usecols=[0, 1])\n",
    "middle_vocabulary_df.columns=[\"word\", \"pos\"]\n",
    "middle_vocabulary = set([w.lower() for w in middle_vocabulary_df.word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_past(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PAST,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def make_progressive(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        aspect = en.PROGRESSIVE,\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    ) \n",
    "\n",
    "\n",
    "def make_perfect(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PAST,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        aspect = en.PARTICIPLE,\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def make_third_person(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 3,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def is_past(verb):\n",
    "    return make_past(verb) == verb\n",
    "\n",
    "\n",
    "def is_progressive(verb):\n",
    "    return make_progressive(verb) == verb\n",
    "\n",
    "\n",
    "def is_perfect(verb):\n",
    "    return make_perfect(verb) == verb\n",
    "\n",
    "\n",
    "def is_third_person(verb):\n",
    "    return make_third_person(verb) == verb\n",
    "\n",
    "\n",
    "def is_verb(pos):\n",
    "    return pos[:1] == \"V\"\n",
    "\n",
    "\n",
    "def is_plural(word):\n",
    "    return en.singularize(word) != word\n",
    "\n",
    "\n",
    "def is_noun(pos):\n",
    "    return pos == \"NN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        tokens_with_space = [tokens[0]]\n",
    "        prev_word = tokens[0]\n",
    "        for token in tokens[1:]:\n",
    "            if (\n",
    "                token not in {\".\", \",\", \"!\", \"?\", \";\", \":\", \")\"} \n",
    "                    and token[:1] != \"'\"\n",
    "                    and prev_word != \"(\"\n",
    "            ):\n",
    "                tokens_with_space.append(\" \")\n",
    "            tokens_with_space.append(token)\n",
    "            prev_word = token\n",
    "        return \"\".join(tokens_with_space)\n",
    "    \n",
    "    \n",
    "def get_form(substitute, original_word, pos):\n",
    "    if is_verb(pos):\n",
    "        if is_past(original_word):\n",
    "            return make_past(substitute)\n",
    "        elif is_progressive(original_word):\n",
    "            return make_progressive(substitute)\n",
    "        elif is_perfect(original_word):\n",
    "            return make_perfect(substitute)\n",
    "        elif is_third_person(original_word):\n",
    "            return make_third_person(substitute)\n",
    "        else:\n",
    "            return substitute\n",
    "    elif is_noun(pos):\n",
    "        if is_plural(original_word):\n",
    "            return en.pluralize(substitute)\n",
    "        else:\n",
    "            return substitute\n",
    "    else:\n",
    "        return substitute\n",
    "    \n",
    "\n",
    "def is_in_vocabulary(synset, vocabulary):\n",
    "    return synset[0] in vocabulary\n",
    "\n",
    "\n",
    "def get_hypernym(token, pos, lemmatized_token, middle_vocabulary):\n",
    "    if is_verb(pos) or is_noun(pos) or pos == \"ADV\" or pos == \"ADJ\":\n",
    "        synsets = en.wordnet.synsets(lemmatized_token, pos=pos)\n",
    "        for synset in synsets:\n",
    "            if is_in_vocabulary(synset, middle_vocabulary):\n",
    "                for hypernym in synsets[0].hypernyms():\n",
    "                    if is_in_vocabulary(hypernym, middle_vocabulary):\n",
    "                        return get_form(hypernym[0], token, pos)\n",
    "    return None\n",
    "    \n",
    "\n",
    "def get_synonym(token, pos, lemmatized_token, synonyms_map):\n",
    "    if lemmatized_token in synonyms_map:\n",
    "        synonym = np.random.choice(synonyms_map[lemmatized_token], 1)[0]\n",
    "        return get_form(synonym, token, pos)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def paraphrase(sentence, synonyms_map, middle_vocabulary, synonym_stats, hypernym_stats):\n",
    "    tokens, pos = zip(*en.tag(sentence))\n",
    "    tokens = list(tokens)\n",
    "    substituted = False\n",
    "    for i in range(len(tokens)):\n",
    "        lemmatized_token = lemma(tokens[i])\n",
    "        synonym = get_synonym( # trying to substitute with a synonym first\n",
    "            tokens[i], pos[i], lemmatized_token, synonyms_map\n",
    "        )\n",
    "        if synonym is not None: \n",
    "            synonym_stats[lemmatized_token] += 1\n",
    "            tokens[i] = synonym\n",
    "            substituted = True\n",
    "        else: # no synonym found, trying to substitute with a hypernym\n",
    "            hypernym = get_hypernym(\n",
    "                tokens[i], pos[i], lemmatized_token, middle_vocabulary\n",
    "            )\n",
    "            if hypernym is not None:\n",
    "                hypernym_stats[lemmatized_token] += 1\n",
    "                tokens[i] = hypernym\n",
    "                substituted = True\n",
    "    return join_tokens(tokens), substituted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He obtained rights to the record.', True)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(\n",
    "    \"He gained access to the file.\", \n",
    "    synonyms_map, \n",
    "    middle_vocabulary,\n",
    "    collections.Counter(),\n",
    "    collections.Counter()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alternatives_df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"alternatives_train-middle_68aa2429.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paraphrase_statements(statements, synonyms_map):\n",
    "    paraphrased_statements = []\n",
    "    substitution_indicators = []\n",
    "    synonym_stats = collections.Counter()\n",
    "    hypernym_stats = collections.Counter()\n",
    "    for statement in statements:\n",
    "        paraphrased_statement, substituted = paraphrase(\n",
    "            statement, synonyms_map, middle_vocabulary, synonym_stats, hypernym_stats\n",
    "        )\n",
    "        paraphrased_statements.append(paraphrased_statement)\n",
    "        substitution_indicators.append(substituted)\n",
    "    return paraphrased_statements, substitution_indicators, synonym_stats, hypernym_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_paraphrased_statement_in_place(df, column, synonyms_map):\n",
    "    paraphrased_statements, substitution_indicators, synonym_stats, hypernym_stats = \\\n",
    "        paraphrase_statements(\n",
    "            df[column], synonyms_map\n",
    "        )\n",
    "    df[f\"paraphrased_{column}\"] = paraphrased_statements\n",
    "    df[f\"was_{column}_paraphrased\"] = substitution_indicators\n",
    "    return synonym_stats, hypernym_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_stats__true_statements, hypernym_stats__true_statements \\\n",
    "    = add_paraphrased_statement_in_place(\n",
    "        alternatives_df, \"true_statement\", synonyms_map\n",
    "    )\n",
    "synonym_stats__alternatives, hypernym_stats__alternatives = \\\n",
    "    add_paraphrased_statement_in_place(\n",
    "        alternatives_df, \"alternative_statement\", synonyms_map\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 2100),\n",
       " ('very', 1530),\n",
       " ('all', 1324),\n",
       " ('good', 883),\n",
       " ('new', 670),\n",
       " ('old', 591),\n",
       " ('learn', 563),\n",
       " ('father', 510),\n",
       " ('big', 473),\n",
       " ('begin', 460),\n",
       " ('buy', 439),\n",
       " ('kind', 428),\n",
       " ('place', 421),\n",
       " ('little', 401),\n",
       " ('love', 387),\n",
       " ('important', 355),\n",
       " ('house', 352),\n",
       " ('away', 351),\n",
       " ('problem', 350),\n",
       " ('shop', 342),\n",
       " ('happy', 329),\n",
       " ('usually', 315),\n",
       " ('word', 273),\n",
       " ('popular', 263),\n",
       " ('enjoy', 259),\n",
       " ('job', 255),\n",
       " ('great', 243),\n",
       " ('answer', 241),\n",
       " ('woman', 237),\n",
       " ('bad', 236),\n",
       " ('part', 232),\n",
       " ('right', 227),\n",
       " ('decide', 214),\n",
       " ('fly', 209),\n",
       " ('beautiful', 205),\n",
       " ('idea', 205),\n",
       " ('street', 183),\n",
       " ('easy', 170),\n",
       " ('angry', 169),\n",
       " ('story', 169),\n",
       " ('earth', 155),\n",
       " ('hurt', 153),\n",
       " ('difficult', 151),\n",
       " ('cry', 147),\n",
       " ('child', 141),\n",
       " ('sure', 134),\n",
       " ('fast', 127),\n",
       " ('cold', 113),\n",
       " ('trip', 102),\n",
       " ('sad', 98),\n",
       " ('arrive', 96),\n",
       " ('strange', 94),\n",
       " ('wrong', 91),\n",
       " ('explain', 88),\n",
       " ('gift', 85),\n",
       " ('quite', 84),\n",
       " ('wonderful', 78),\n",
       " ('kill', 77),\n",
       " ('fresh', 74),\n",
       " ('dark', 70),\n",
       " ('hide', 68),\n",
       " ('mark', 65),\n",
       " ('stone', 62),\n",
       " ('round', 61),\n",
       " ('sick', 59),\n",
       " ('quiet', 59),\n",
       " ('bright', 58),\n",
       " ('nearly', 57),\n",
       " ('center', 49),\n",
       " ('trouble', 49),\n",
       " ('rest', 48),\n",
       " ('coat', 48),\n",
       " ('dear', 48),\n",
       " ('taxi', 45),\n",
       " ('funny', 40),\n",
       " ('receive', 40),\n",
       " ('lucky', 39),\n",
       " ('shy', 38),\n",
       " ('page', 37),\n",
       " ('loud', 36),\n",
       " ('pretty', 34),\n",
       " ('describe', 32),\n",
       " ('raise', 31),\n",
       " ('fat', 31),\n",
       " ('hat', 31),\n",
       " ('unhappy', 24),\n",
       " ('complete', 24),\n",
       " ('hate', 23),\n",
       " ('smart', 23),\n",
       " ('fair', 21),\n",
       " ('below', 21),\n",
       " ('business', 20),\n",
       " ('value', 20),\n",
       " ('writer', 19),\n",
       " ('destroy', 18),\n",
       " ('ugly', 17),\n",
       " ('perform', 17),\n",
       " ('speech', 15),\n",
       " ('extra', 13),\n",
       " ('amount', 12),\n",
       " ('suggest', 11),\n",
       " ('respond', 11),\n",
       " ('total', 11),\n",
       " ('fantastic', 11),\n",
       " ('brave', 10),\n",
       " ('slim', 9),\n",
       " ('jungle', 8),\n",
       " ('accurate', 7),\n",
       " ('alike', 6),\n",
       " ('former', 6),\n",
       " ('behave', 6),\n",
       " ('eager', 4),\n",
       " ('bother', 4),\n",
       " ('trust', 4),\n",
       " ('pause', 1),\n",
       " ('opportunity', 1),\n",
       " ('blank', 1)]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_stats__true_statements.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fact', 5550),\n",
       " ('time', 1207),\n",
       " ('come', 1029),\n",
       " ('take', 975),\n",
       " ('help', 831),\n",
       " ('man', 751),\n",
       " ('ask', 673),\n",
       " ('tell', 670),\n",
       " ('mother', 657),\n",
       " ('play', 583),\n",
       " ('way', 553),\n",
       " ('water', 499),\n",
       " ('call', 491),\n",
       " ('try', 488),\n",
       " ('read', 459),\n",
       " ('eat', 453),\n",
       " ('live', 436),\n",
       " ('talk', 409),\n",
       " ('family', 389),\n",
       " ('walk', 370),\n",
       " ('put', 354),\n",
       " ('boy', 350),\n",
       " ('stay', 304),\n",
       " ('show', 303),\n",
       " ('grow', 291),\n",
       " ('work', 275),\n",
       " ('girl', 240),\n",
       " ('turn', 237),\n",
       " ('fall', 233),\n",
       " ('teach', 219),\n",
       " ('room', 219),\n",
       " ('seem', 212),\n",
       " ('stand', 207),\n",
       " ('friend', 193),\n",
       " ('thing', 192),\n",
       " ('one', 185),\n",
       " ('book', 183),\n",
       " ('visit', 183),\n",
       " ('class', 183),\n",
       " ('air', 178),\n",
       " ('computer', 175),\n",
       " ('believe', 171),\n",
       " ('paper', 171),\n",
       " ('sleep', 162),\n",
       " ('birthday', 160),\n",
       " ('number', 148),\n",
       " ('choose', 144),\n",
       " ('bring', 137),\n",
       " ('party', 137),\n",
       " ('exercise', 136),\n",
       " ('team', 136),\n",
       " ('dance', 136),\n",
       " ('finish', 135),\n",
       " ('send', 127),\n",
       " ('today', 126),\n",
       " ('lunch', 125),\n",
       " ('baby', 121),\n",
       " ('language', 121),\n",
       " ('care', 120),\n",
       " ('pick', 120),\n",
       " ('build', 118),\n",
       " ('sell', 116),\n",
       " ('save', 116),\n",
       " ('pass', 116),\n",
       " ('wife', 113),\n",
       " ('mom', 112),\n",
       " ('pay', 111),\n",
       " ('dream', 111),\n",
       " ('desk', 111),\n",
       " ('afternoon', 109),\n",
       " ('speak', 109),\n",
       " ('protect', 109),\n",
       " ('game', 107),\n",
       " ('road', 107),\n",
       " ('return', 106),\n",
       " ('drive', 105),\n",
       " ('hope', 104),\n",
       " ('winter', 102),\n",
       " ('report', 101),\n",
       " ('follow', 101),\n",
       " ('climb', 100),\n",
       " ('notice', 99),\n",
       " ('develop', 99),\n",
       " ('breakfast', 97),\n",
       " ('dinner', 97),\n",
       " ('traffic', 94),\n",
       " ('test', 93),\n",
       " ('information', 93),\n",
       " ('history', 91),\n",
       " ('front', 91),\n",
       " ('clean', 91),\n",
       " ('ride', 90),\n",
       " ('summer', 90),\n",
       " ('set', 90),\n",
       " ('swim', 88),\n",
       " ('chair', 87),\n",
       " ('carry', 87),\n",
       " ('catch', 87),\n",
       " ('side', 86),\n",
       " ('improve', 86),\n",
       " ('plan', 86),\n",
       " ('technology', 86),\n",
       " ('break', 85),\n",
       " ('sing', 85),\n",
       " ('even', 83),\n",
       " ('age', 81),\n",
       " ('use', 80),\n",
       " ('study', 80),\n",
       " ('holiday', 78),\n",
       " ('appear', 74),\n",
       " ('wash', 74),\n",
       " ('lady', 73),\n",
       " ('sound', 72),\n",
       " ('driver', 72),\n",
       " ('born', 71),\n",
       " ('environment', 71),\n",
       " ('burn', 70),\n",
       " ('cause', 69),\n",
       " ('sign', 69),\n",
       " ('grandmother', 68),\n",
       " ('waste', 68),\n",
       " ('letter', 67),\n",
       " ('draw', 66),\n",
       " ('chance', 66),\n",
       " ('cut', 66),\n",
       " ('offer', 66),\n",
       " ('noise', 65),\n",
       " ('culture', 65),\n",
       " ('river', 65),\n",
       " ('experience', 65),\n",
       " ('refuse', 63),\n",
       " ('rise', 62),\n",
       " ('lie', 62),\n",
       " ('exam', 62),\n",
       " ('collect', 62),\n",
       " ('area', 61),\n",
       " ('hero', 61),\n",
       " ('piece', 61),\n",
       " ('supper', 60),\n",
       " ('practice', 60),\n",
       " ('glass', 60),\n",
       " ('grandfather', 59),\n",
       " ('pull', 59),\n",
       " ('difference', 59),\n",
       " ('need', 57),\n",
       " ('top', 57),\n",
       " ('chat', 57),\n",
       " ('jump', 57),\n",
       " ('sky', 57),\n",
       " ('prepare', 57),\n",
       " ('newspaper', 56),\n",
       " ('drop', 55),\n",
       " ('dad', 54),\n",
       " ('form', 54),\n",
       " ('encourage', 54),\n",
       " ('future', 53),\n",
       " ('produce', 53),\n",
       " ('realize', 53),\n",
       " ('order', 53),\n",
       " ('reply', 52),\n",
       " ('restaurant', 52),\n",
       " ('depend', 51),\n",
       " ('influence', 51),\n",
       " ('discover', 50),\n",
       " ('hotel', 50),\n",
       " ('kitchen', 50),\n",
       " ('voice', 50),\n",
       " ('respect', 50),\n",
       " ('meat', 50),\n",
       " ('add', 50),\n",
       " ('stop', 49),\n",
       " ('point', 49),\n",
       " ('movie', 49),\n",
       " ('matter', 48),\n",
       " ('college', 48),\n",
       " ('subject', 47),\n",
       " ('mean', 47),\n",
       " ('continue', 47),\n",
       " ('program', 46),\n",
       " ('type', 46),\n",
       " ('film', 45),\n",
       " ('paint', 45),\n",
       " ('match', 45),\n",
       " ('adult', 45),\n",
       " ('wonder', 45),\n",
       " ('hall', 45),\n",
       " ('magazine', 45),\n",
       " ('card', 44),\n",
       " ('wind', 43),\n",
       " ('speed', 42),\n",
       " ('name', 42),\n",
       " ('farm', 42),\n",
       " ('library', 42),\n",
       " ('relationship', 41),\n",
       " ('scientist', 40),\n",
       " ('copy', 40),\n",
       " ('consider', 40),\n",
       " ('shout', 40),\n",
       " ('art', 40),\n",
       " ('hole', 40),\n",
       " ('danger', 40),\n",
       " ('luck', 40),\n",
       " ('middle', 39),\n",
       " ('heat', 39),\n",
       " ('check', 39),\n",
       " ('touch', 38),\n",
       " ('cross', 38),\n",
       " ('cake', 37),\n",
       " ('provide', 36),\n",
       " ('death', 36),\n",
       " ('classroom', 36),\n",
       " ('solve', 36),\n",
       " ('seat', 36),\n",
       " ('list', 36),\n",
       " ('affect', 36),\n",
       " ('owner', 36),\n",
       " ('grandma', 36),\n",
       " ('moment', 36),\n",
       " ('shake', 35),\n",
       " ('ability', 35),\n",
       " ('advice', 35),\n",
       " ('island', 35),\n",
       " ('entrance', 35),\n",
       " ('math', 34),\n",
       " ('role', 34),\n",
       " ('course', 33),\n",
       " ('nature', 33),\n",
       " ('rubbish', 33),\n",
       " ('safety', 33),\n",
       " ('power', 33),\n",
       " ('fill', 33),\n",
       " ('boss', 33),\n",
       " ('corner', 33),\n",
       " ('introduce', 33),\n",
       " ('law', 33),\n",
       " ('challenge', 33),\n",
       " ('couple', 32),\n",
       " ('publish', 32),\n",
       " ('situation', 32),\n",
       " ('passion', 32),\n",
       " ('rush', 31),\n",
       " ('lift', 31),\n",
       " ('colour', 31),\n",
       " ('action', 31),\n",
       " ('best', 30),\n",
       " ('sang', 30),\n",
       " ('train', 30),\n",
       " ('lock', 30),\n",
       " ('bottom', 30),\n",
       " ('vacation', 29),\n",
       " ('arrange', 29),\n",
       " ('purpose', 28),\n",
       " ('bedroom', 28),\n",
       " ('education', 28),\n",
       " ('activity', 28),\n",
       " ('excuse', 28),\n",
       " ('service', 27),\n",
       " ('goal', 27),\n",
       " ('tonight', 27),\n",
       " ('steal', 27),\n",
       " ('lay', 26),\n",
       " ('task', 26),\n",
       " ('fashion', 26),\n",
       " ('favourite', 26),\n",
       " ('cost', 26),\n",
       " ('race', 26),\n",
       " ('price', 25),\n",
       " ('move', 25),\n",
       " ('push', 25),\n",
       " ('feed', 25),\n",
       " ('present', 24),\n",
       " ('surprise', 24),\n",
       " ('postcard', 24),\n",
       " ('effort', 24),\n",
       " ('level', 24),\n",
       " ('calm', 24),\n",
       " ('admire', 24),\n",
       " ('yesterday', 24),\n",
       " ('surf', 23),\n",
       " ('expert', 23),\n",
       " ('conversation', 23),\n",
       " ('friendship', 23),\n",
       " ('sir', 23),\n",
       " ('praise', 23),\n",
       " ('spring', 23),\n",
       " ('beauty', 23),\n",
       " ('bookstore', 23),\n",
       " ('jog', 23),\n",
       " ('tomorrow', 22),\n",
       " ('pair', 22),\n",
       " ('bathroom', 22),\n",
       " ('housework', 22),\n",
       " ('playground', 22),\n",
       " ('snack', 22),\n",
       " ('manager', 22),\n",
       " ('example', 22),\n",
       " ('secret', 21),\n",
       " ('blackboard', 21),\n",
       " ('worker', 21),\n",
       " ('hand', 21),\n",
       " ('argue', 21),\n",
       " ('weigh', 21),\n",
       " ('score', 21),\n",
       " ('message', 21),\n",
       " ('jacket', 21),\n",
       " ('factory', 21),\n",
       " ('welcome', 21),\n",
       " ('university', 20),\n",
       " ('treatment', 20),\n",
       " ('countryside', 20),\n",
       " ('guess', 20),\n",
       " ('lesson', 20),\n",
       " ('slow', 20),\n",
       " ('bridge', 20),\n",
       " ('neighbor', 20),\n",
       " ('reduce', 20),\n",
       " ('member', 20),\n",
       " ('truth', 20),\n",
       " ('habit', 20),\n",
       " ('development', 20),\n",
       " ('tie', 19),\n",
       " ('roll', 19),\n",
       " ('police', 19),\n",
       " ('journey', 19),\n",
       " ('ceremony', 19),\n",
       " ('painter', 19),\n",
       " ('search', 19),\n",
       " ('board', 19),\n",
       " ('clerk', 19),\n",
       " ('tap', 19),\n",
       " ('coach', 19),\n",
       " ('church', 18),\n",
       " ('stamp', 18),\n",
       " ('text', 18),\n",
       " ('crack', 18),\n",
       " ('dress', 18),\n",
       " ('fix', 18),\n",
       " ('pet', 18),\n",
       " ('project', 18),\n",
       " ('measure', 18),\n",
       " ('achieve', 18),\n",
       " ('sofa', 18),\n",
       " ('manage', 17),\n",
       " ('silence', 17),\n",
       " ('outside', 17),\n",
       " ('injure', 17),\n",
       " ('patient', 17),\n",
       " ('plant', 17),\n",
       " ('struggle', 17),\n",
       " ('soup', 17),\n",
       " ('singer', 16),\n",
       " ('record', 16),\n",
       " ('permission', 16),\n",
       " ('repeat', 16),\n",
       " ('autumn', 16),\n",
       " ('bookcase', 16),\n",
       " ('promise', 16),\n",
       " ('focus', 15),\n",
       " ('pop', 15),\n",
       " ('decorate', 15),\n",
       " ('pleasure', 15),\n",
       " ('diet', 15),\n",
       " ('invitation', 15),\n",
       " ('master', 15),\n",
       " ('style', 15),\n",
       " ('tooth', 15),\n",
       " ('signal', 15),\n",
       " ('prefer', 15),\n",
       " ('ring', 15),\n",
       " ('blame', 15),\n",
       " ('dig', 15),\n",
       " ('exhibition', 15),\n",
       " ('cousin', 15),\n",
       " ('importance', 15),\n",
       " ('picnic', 15),\n",
       " ('prove', 15),\n",
       " ('public', 14),\n",
       " ('businessman', 14),\n",
       " ('paid', 14),\n",
       " ('programme', 14),\n",
       " ('warn', 14),\n",
       " ('knock', 14),\n",
       " ('afford', 14),\n",
       " ('deliver', 14),\n",
       " ('market', 14),\n",
       " ('path', 14),\n",
       " ('article', 14),\n",
       " ('protection', 14),\n",
       " ('grandpa', 14),\n",
       " ('wildlife', 14),\n",
       " ('get', 14),\n",
       " ('regard', 14),\n",
       " ('parrot', 14),\n",
       " ('thirst', 14),\n",
       " ('actor', 13),\n",
       " ('plastic', 13),\n",
       " ('peace', 13),\n",
       " ('branch', 13),\n",
       " ('double', 13),\n",
       " ('press', 13),\n",
       " ('wine', 13),\n",
       " ('ease', 13),\n",
       " ('theater', 13),\n",
       " ('last', 13),\n",
       " ('assistant', 13),\n",
       " ('engine', 13),\n",
       " ('stick', 13),\n",
       " ('compare', 13),\n",
       " ('shame', 13),\n",
       " ('admit', 13),\n",
       " ('diary', 13),\n",
       " ('attract', 12),\n",
       " ('repair', 12),\n",
       " ('shock', 12),\n",
       " ('engineer', 12),\n",
       " ('concert', 12),\n",
       " ('control', 12),\n",
       " ('theatre', 12),\n",
       " ('cinema', 12),\n",
       " ('musician', 12),\n",
       " ('advertise', 12),\n",
       " ('decision', 12),\n",
       " ('toilet', 12),\n",
       " ('drown', 12),\n",
       " ('finger', 12),\n",
       " ('row', 12),\n",
       " ('mankind', 12),\n",
       " ('tower', 12),\n",
       " ('source', 12),\n",
       " ('tour', 12),\n",
       " ('grandson', 12),\n",
       " ('involve', 12),\n",
       " ('credit', 12),\n",
       " ('packet', 12),\n",
       " ('fine', 12),\n",
       " ('condition', 11),\n",
       " ('skateboard', 11),\n",
       " ('bar', 11),\n",
       " ('inside', 11),\n",
       " ('rang', 11),\n",
       " ('mind', 11),\n",
       " ('john', 11),\n",
       " ('percent', 11),\n",
       " ('wheel', 11),\n",
       " ('harbor', 11),\n",
       " ('prize', 11),\n",
       " ('step', 11),\n",
       " ('shelter', 11),\n",
       " ('shoot', 11),\n",
       " ('topic', 11),\n",
       " ('career', 11),\n",
       " ('corn', 11),\n",
       " ('ad', 10),\n",
       " ('founder', 10),\n",
       " ('intelligence', 10),\n",
       " ('process', 10),\n",
       " ('alarm', 10),\n",
       " ('joke', 10),\n",
       " ('make', 10),\n",
       " ('tiger', 10),\n",
       " ('chemist', 10),\n",
       " ('launch', 10),\n",
       " ('documentary', 10),\n",
       " ('principal', 10),\n",
       " ('observe', 10),\n",
       " ('term', 10),\n",
       " ('bump', 10),\n",
       " ('neighborhood', 10),\n",
       " ('translate', 10),\n",
       " ('request', 10),\n",
       " ('gentleman', 9),\n",
       " ('suggestion', 9),\n",
       " ('degree', 9),\n",
       " ('thief', 9),\n",
       " ('salad', 9),\n",
       " ('layer', 9),\n",
       " ('customer', 9),\n",
       " ('wheelchair', 9),\n",
       " ('earn', 9),\n",
       " ('start', 9),\n",
       " ('disease', 9),\n",
       " ('rob', 9),\n",
       " ('lab', 9),\n",
       " ('menu', 9),\n",
       " ('clap', 9),\n",
       " ('calendar', 9),\n",
       " ('star', 9),\n",
       " ('honour', 9),\n",
       " ('guard', 9),\n",
       " ('partner', 9),\n",
       " ('don', 9),\n",
       " ('ski', 9),\n",
       " ('curry', 9),\n",
       " ('crop', 9),\n",
       " ('album', 8),\n",
       " ('progress', 8),\n",
       " ('educate', 8),\n",
       " ('vegetable', 8),\n",
       " ('borrow', 8),\n",
       " ('backpack', 8),\n",
       " ('threat', 8),\n",
       " ('result', 8),\n",
       " ('enjoyment', 8),\n",
       " ('performance', 8),\n",
       " ('dry', 8),\n",
       " ('thumb', 8),\n",
       " ('railway', 8),\n",
       " ('date', 8),\n",
       " ('kite', 8),\n",
       " ('none', 8),\n",
       " ('location', 8),\n",
       " ('communication', 8),\n",
       " ('clear', 8),\n",
       " ('uniform', 8),\n",
       " ('ruler', 8),\n",
       " ('park', 8),\n",
       " ('cheese', 8),\n",
       " ('direct', 8),\n",
       " ('balcony', 8),\n",
       " ('do', 8),\n",
       " ('jazz', 8),\n",
       " ('encouragement', 8),\n",
       " ('sting', 8),\n",
       " ('attend', 7),\n",
       " ('charge', 7),\n",
       " ('scream', 7),\n",
       " ('strike', 7),\n",
       " ('novel', 7),\n",
       " ('criticize', 7),\n",
       " ('designer', 7),\n",
       " ('beat', 7),\n",
       " ('pour', 7),\n",
       " ('store', 7),\n",
       " ('madam', 7),\n",
       " ('statue', 7),\n",
       " ('risk', 7),\n",
       " ('breeze', 7),\n",
       " ('exit', 7),\n",
       " ('collection', 7),\n",
       " ('skill', 7),\n",
       " ('mail', 7),\n",
       " ('pile', 7),\n",
       " ('land', 7),\n",
       " ('guide', 7),\n",
       " ('past', 7),\n",
       " ('leader', 7),\n",
       " ('architecture', 7),\n",
       " ('retreat', 6),\n",
       " ('scene', 6),\n",
       " ('granny', 6),\n",
       " ('host', 6),\n",
       " ('survey', 6),\n",
       " ('hike', 6),\n",
       " ('suffer', 6),\n",
       " ('harvest', 6),\n",
       " ('birth', 6),\n",
       " ('lack', 6),\n",
       " ('men', 6),\n",
       " ('wait', 6),\n",
       " ('difficulty', 6),\n",
       " ('trail', 6),\n",
       " ('recycle', 6),\n",
       " ('review', 6),\n",
       " ('display', 6),\n",
       " ('damage', 6),\n",
       " ('select', 6),\n",
       " ('comfort', 6),\n",
       " ('direction', 6),\n",
       " ('pattern', 6),\n",
       " ('noon', 6),\n",
       " ('remain', 6),\n",
       " ('fund', 6),\n",
       " ('cow', 6),\n",
       " ('upset', 6),\n",
       " ('steam', 6),\n",
       " ('inn', 6),\n",
       " ('print', 6),\n",
       " ('cash', 6),\n",
       " ('win', 6),\n",
       " ('judge', 6),\n",
       " ('color', 6),\n",
       " ('trade', 6),\n",
       " ('tailor', 6),\n",
       " ('notebook', 6),\n",
       " ('chemical', 6),\n",
       " ('textbook', 6),\n",
       " ('satisfy', 6),\n",
       " ('stress', 6),\n",
       " ('teatime', 6),\n",
       " ('pork', 6),\n",
       " ('enroll', 6),\n",
       " ('occupation', 6),\n",
       " ('warm', 6),\n",
       " ('delay', 6),\n",
       " ('click', 6),\n",
       " ('vase', 6),\n",
       " ('slice', 6),\n",
       " ('candle', 6),\n",
       " ('canal', 6),\n",
       " ('thunder', 6),\n",
       " ('recognize', 6),\n",
       " ('photograph', 6),\n",
       " ('rely', 6),\n",
       " ('anniversary', 6),\n",
       " ('specialist', 6),\n",
       " ('clown', 6),\n",
       " ('conclude', 6),\n",
       " ('romance', 6),\n",
       " ('snowboard', 6),\n",
       " ('literature', 6),\n",
       " ('post', 5),\n",
       " ('rent', 5),\n",
       " ('midnight', 5),\n",
       " ('left', 5),\n",
       " ('fit', 5),\n",
       " ('close', 5),\n",
       " ('shape', 5),\n",
       " ('support', 5),\n",
       " ('hunger', 5),\n",
       " ('approach', 5),\n",
       " ('aim', 5),\n",
       " ('campaign', 5),\n",
       " ('recite', 5),\n",
       " ('recipe', 5),\n",
       " ('see', 5),\n",
       " ('issue', 5),\n",
       " ('web', 5),\n",
       " ('trash', 5),\n",
       " ('warmth', 5),\n",
       " ('ache', 5),\n",
       " ('contain', 5),\n",
       " ('wallet', 5),\n",
       " ('open', 5),\n",
       " ('kit', 5),\n",
       " ('studio', 4),\n",
       " ('origin', 4),\n",
       " ('neighbour', 4),\n",
       " ('limit', 4),\n",
       " ('paragraph', 4),\n",
       " ('concern', 4),\n",
       " ('impact', 4),\n",
       " ('factor', 4),\n",
       " ('compute', 4),\n",
       " ('compliment', 4),\n",
       " ('update', 4),\n",
       " ('motor', 4),\n",
       " ('note', 4),\n",
       " ('banana', 4),\n",
       " ('decrease', 4),\n",
       " ('toothbrush', 4),\n",
       " ('lend', 4),\n",
       " ('mama', 4),\n",
       " ('primary', 4),\n",
       " ('reality', 4),\n",
       " ('attraction', 4),\n",
       " ('guest', 4),\n",
       " ('iron', 4),\n",
       " ('plain', 4),\n",
       " ('conference', 4),\n",
       " ('strength', 4),\n",
       " ('theme', 4),\n",
       " ('connection', 4),\n",
       " ('stock', 4),\n",
       " ('sail', 4),\n",
       " ('interpret', 4),\n",
       " ('dessert', 4),\n",
       " ('erupt', 4),\n",
       " ('vision', 4),\n",
       " ('guidebook', 4),\n",
       " ('recover', 4),\n",
       " ('discussion', 4),\n",
       " ('install', 4),\n",
       " ('stretch', 4),\n",
       " ('cool', 4),\n",
       " ('production', 4),\n",
       " ('comedy', 4),\n",
       " ('security', 4),\n",
       " ('section', 4),\n",
       " ('armchair', 4),\n",
       " ('highway', 4),\n",
       " ('stadium', 4),\n",
       " ('folk', 4),\n",
       " ('fighter', 4),\n",
       " ('sweep', 4),\n",
       " ('queen', 4),\n",
       " ('honor', 4),\n",
       " ('jet', 4),\n",
       " ('spot', 4),\n",
       " ('practise', 4),\n",
       " ('volume', 4),\n",
       " ('fare', 3),\n",
       " ('operation', 3),\n",
       " ('graduate', 3),\n",
       " ('drag', 3),\n",
       " ('track', 3),\n",
       " ('preparation', 3),\n",
       " ('advertisement', 3),\n",
       " ('reward', 3),\n",
       " ('decline', 3),\n",
       " ('delivery', 3),\n",
       " ('blogger', 3),\n",
       " ('mend', 3),\n",
       " ('divorce', 3),\n",
       " ('float', 3),\n",
       " ('colleague', 3),\n",
       " ('champion', 3),\n",
       " ('explanation', 3),\n",
       " ('operate', 3),\n",
       " ('picture', 3),\n",
       " ('appreciate', 3),\n",
       " ('judgment', 3),\n",
       " ('brand', 3),\n",
       " ('wander', 3),\n",
       " ('rule', 3),\n",
       " ('pirate', 3),\n",
       " ('trend', 3),\n",
       " ('girlfriend', 3),\n",
       " ('sport', 3),\n",
       " ('lick', 3),\n",
       " ('greet', 3),\n",
       " ('railroad', 3),\n",
       " ('escalator', 3),\n",
       " ('bookshop', 3),\n",
       " ('bear', 3),\n",
       " ('bug', 3),\n",
       " ('lave', 3),\n",
       " ('back', 3),\n",
       " ('dirt', 3),\n",
       " ('convenience', 3),\n",
       " ('strengthen', 3),\n",
       " ('expand', 3),\n",
       " ('sportsman', 3),\n",
       " ('declare', 3),\n",
       " ('package', 3),\n",
       " ('consumer', 3),\n",
       " ('trial', 3),\n",
       " ('resort', 3),\n",
       " ('invest', 3),\n",
       " ('concentration', 3),\n",
       " ('cucumber', 3),\n",
       " ('chief', 3),\n",
       " ('frost', 3),\n",
       " ('out', 3),\n",
       " ('rap', 3),\n",
       " ('dine', 2),\n",
       " ('neighbourhood', 2),\n",
       " ('velocity', 2),\n",
       " ('custom', 2),\n",
       " ('line', 2),\n",
       " ('state', 2),\n",
       " ('melody', 2),\n",
       " ('slip', 2),\n",
       " ('channel', 2),\n",
       " ('question', 2),\n",
       " ('deserve', 2),\n",
       " ('prescription', 2),\n",
       " ('crowd', 2),\n",
       " ('congratulate', 2),\n",
       " ('route', 2),\n",
       " ('usage', 2),\n",
       " ('lover', 2),\n",
       " ('instruction', 2),\n",
       " ('fear', 2),\n",
       " ('unknown', 2),\n",
       " ('label', 2),\n",
       " ('content', 2),\n",
       " ('toy', 2),\n",
       " ('subtract', 2),\n",
       " ('resolve', 2),\n",
       " ('toast', 2),\n",
       " ('endure', 2),\n",
       " ('curtain', 2),\n",
       " ('secretary', 2),\n",
       " ('kiss', 2),\n",
       " ('tutor', 2),\n",
       " ('bury', 2),\n",
       " ('association', 2),\n",
       " ('tune', 2),\n",
       " ('reception', 2),\n",
       " ('butter', 2),\n",
       " ('repay', 2),\n",
       " ('firm', 2),\n",
       " ('ladder', 2),\n",
       " ('guy', 2),\n",
       " ('user', 2),\n",
       " ('upside', 2),\n",
       " ('run', 2),\n",
       " ('chore', 2),\n",
       " ('fee', 2),\n",
       " ('circle', 2),\n",
       " ('clue', 2),\n",
       " ('fish', 2),\n",
       " ('salesman', 2),\n",
       " ('laboratory', 2),\n",
       " ('vote', 2),\n",
       " ('lawyer', 2),\n",
       " ('generation', 2),\n",
       " ('operator', 2),\n",
       " ('destination', 2),\n",
       " ('settle', 2),\n",
       " ('download', 2),\n",
       " ('network', 2),\n",
       " ('tournament', 2),\n",
       " ('population', 2),\n",
       " ('threaten', 2),\n",
       " ('assemble', 2),\n",
       " ('milk', 2),\n",
       " ('miss', 2),\n",
       " ('victory', 2),\n",
       " ('impression', 2),\n",
       " ('journal', 2),\n",
       " ('beef', 2),\n",
       " ('bend', 2),\n",
       " ('bet', 2),\n",
       " ('coast', 2),\n",
       " ('daddy', 2),\n",
       " ('housewife', 2),\n",
       " ('pillow', 2),\n",
       " ('combine', 2),\n",
       " ('celebration', 2),\n",
       " ('killer', 2),\n",
       " ('evolution', 2),\n",
       " ('campground', 2),\n",
       " ('white', 2),\n",
       " ('puzzle', 2),\n",
       " ('demand', 2),\n",
       " ('locate', 2),\n",
       " ('businesswoman', 2),\n",
       " ('outsider', 1),\n",
       " ('registration', 1),\n",
       " ('runner', 1),\n",
       " ('couch', 1),\n",
       " ('equip', 1),\n",
       " ('servant', 1),\n",
       " ('grant', 1),\n",
       " ('disturbance', 1),\n",
       " ('reuse', 1),\n",
       " ('lower', 1),\n",
       " ('puppy', 1),\n",
       " ('count', 1),\n",
       " ('fountain', 1),\n",
       " ('teamwork', 1),\n",
       " ('force', 1),\n",
       " ('sue', 1),\n",
       " ('surname', 1),\n",
       " ('bookshelf', 1),\n",
       " ('shelf', 1),\n",
       " ('mouth', 1),\n",
       " ('account', 1),\n",
       " ('compromise', 1),\n",
       " ('restart', 1),\n",
       " ('high', 1),\n",
       " ('organize', 1),\n",
       " ('regret', 1),\n",
       " ('rejection', 1),\n",
       " ('trainer', 1),\n",
       " ('squeeze', 1),\n",
       " ('journalist', 1),\n",
       " ('surround', 1),\n",
       " ('extend', 1),\n",
       " ('near', 1),\n",
       " ('fellow', 1),\n",
       " ('recovery', 1),\n",
       " ('stain', 1),\n",
       " ('flow', 1),\n",
       " ('rebuild', 1),\n",
       " ('wrapped', 1),\n",
       " ('pedal', 1),\n",
       " ('detail', 1),\n",
       " ('selection', 1),\n",
       " ('bound', 1),\n",
       " ('fasten', 1),\n",
       " ('quick', 1)]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym_stats__true_statements.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936298913334404"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    alternatives_df.loc[:, \"was_true_statement_paraphrased\"]\n",
    ") / len(alternatives_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = alternatives_df[\n",
    "    [\n",
    "        \"text_no\",\n",
    "        \"true_statement\",\n",
    "        \"paraphrased_true_statement\",\n",
    "        \"alternative_statement\",\n",
    "        \"paraphrased_alternative_statement\",\n",
    "        \"was_true_statement_paraphrased\",\n",
    "        \"was_alternative_statement_paraphrased\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Drink the water that has not been boiled because many people think boiled water is safe and good to people's health.\""
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.iloc[0][\"true_statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Drink the liquid that has not been boiled because many people think boiled liquid is safe and satisfactory to people's health.\""
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.iloc[0][\"paraphrased_true_statement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"paraphrased_{RACE_PART.replace('/', '-')}_{random.randint(0, 2**32):x}.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    W2V_MODEL_DIR = \"../w2v_models\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            os.path.join(Word2VecModel.W2V_MODEL_DIR, f\"{model_name}.txt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_nolg_100d = Word2VecModel(\"enwiki_20180420_nolg_100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENTITY/Stour_and_Orwell_Walk', 0.732032835483551),\n",
       " ('path', 0.7227527499198914),\n",
       " ('führerweg', 0.7152807712554932),\n",
       " ('hopefully', 0.7044017314910889),\n",
       " ('ENTITY/WP:EL', 0.7037861347198486),\n",
       " ('ENTITY/WP:DR', 0.7026253938674927),\n",
       " ('jitler', 0.7024155855178833),\n",
       " ('cenzar', 0.7017824649810791),\n",
       " ('torpel', 0.7011029720306396),\n",
       " ('ENTITY/WP:Wikipedia', 0.7005131244659424)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('persons', 0.7992599010467529),\n",
       " ('residents', 0.7535831928253174),\n",
       " ('ENTITY/People', 0.7503395080566406),\n",
       " ('helsinkians', 0.7265914678573608),\n",
       " ('citizens', 0.7153019905090332),\n",
       " ('natives', 0.6979749798774719),\n",
       " ('youths', 0.6972818374633789),\n",
       " ('inhabitants', 0.694731593132019),\n",
       " ('tangbotens', 0.6896241903305054),\n",
       " ('americans', 0.6892567873001099)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENTITY/Chocolate', 0.9106836318969727),\n",
       " ('caramel', 0.8976747989654541),\n",
       " ('ENTITY/Caramel', 0.8792448043823242),\n",
       " ('ENTITY/White_chocolate', 0.8668451309204102),\n",
       " ('ENTITY/Peanut_butter', 0.8533239364624023),\n",
       " ('ENTITY/Types_of_chocolate', 0.8502905368804932),\n",
       " ('chocolates', 0.8490653038024902),\n",
       " ('ENTITY/Types_of_chocolate#Milk_chocolate', 0.8471919298171997),\n",
       " ('candy', 0.844744086265564),\n",
       " ('cream', 0.842796266078949)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"chocolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canceling', 0.8277071714401245),\n",
       " ('cancelling', 0.8080153465270996),\n",
       " ('postpone', 0.7998612523078918),\n",
       " ('reschedule', 0.7844353914260864),\n",
       " ('suspend', 0.7557451725006104),\n",
       " ('rescind', 0.7517332434654236),\n",
       " ('cancellation', 0.7479344010353088),\n",
       " ('cancels', 0.7384265661239624),\n",
       " ('announce', 0.7238617539405823),\n",
       " ('reconsider', 0.7106267213821411),\n",
       " ('renew', 0.7019257545471191),\n",
       " ('discontinue', 0.6998554468154907),\n",
       " ('postponing', 0.6976670026779175),\n",
       " ('postponement', 0.6904169917106628),\n",
       " ('ENTITY/Postpone_to_a_certain_time', 0.6869893670082092),\n",
       " ('announcing', 0.679280161857605),\n",
       " ('reinstate', 0.6781035661697388),\n",
       " ('revoke', 0.6780909299850464),\n",
       " ('rescheduling', 0.6767849922180176),\n",
       " ('renegotiate', 0.6732800602912903)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_nolg_100d.model.most_similar(\"cancel\", topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
