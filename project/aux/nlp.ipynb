{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pattern.en as en\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Constituency = namedtuple(\"Constituency\", [\"tag\", \"startIndex\", \"endIndex\", \"depth\"])\n",
    "\n",
    "\n",
    "def skip_blank_space(parse, pointer):\n",
    "    while pointer < len(parse) and parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return pointer\n",
    "\n",
    "\n",
    "def read_tag(parse, pointer):\n",
    "    tag_start = pointer\n",
    "    while pointer < len(parse) and not parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return parse[tag_start:pointer], skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_token(parse, pointer, tokens):\n",
    "    token_start = pointer\n",
    "    while pointer < len(parse) and parse[pointer] != \")\":\n",
    "        pointer += 1\n",
    "    tokens.append(parse[token_start:pointer])\n",
    "    return pointer\n",
    "    \n",
    "    \n",
    "def read_body(parse, pointer, constituencies, tokens, depth):\n",
    "    if parse[pointer] == \"(\":\n",
    "        return read_constituency(parse, pointer, constituencies, tokens, depth)\n",
    "    else:\n",
    "        return None, read_token(parse, pointer, tokens)\n",
    "        \n",
    "        \n",
    "def read_constituency(parse, pointer, constituencies, tokens, depth):\n",
    "    assert parse[pointer] == \"(\"\n",
    "    pointer += 1\n",
    "    tag, pointer = read_tag(parse, pointer)\n",
    "    first_child, pointer = read_body(parse, pointer, constituencies, tokens, depth + 1)\n",
    "    if first_child is None:\n",
    "        constituency = Constituency(\n",
    "            tag=tag, startIndex=len(tokens), endIndex=len(tokens) + 1, depth=depth\n",
    "        )\n",
    "        assert parse[pointer] == \")\"\n",
    "    else:\n",
    "        child = first_child\n",
    "        while parse[pointer] != \")\":\n",
    "            child, pointer = read_body(parse, pointer, constituencies, tokens, depth + 1)\n",
    "        constituency = Constituency(\n",
    "            tag=tag, startIndex=first_child.startIndex, endIndex=child.endIndex, depth=depth\n",
    "        )\n",
    "    pointer += 1\n",
    "    constituencies.append(constituency)  \n",
    "    return constituency, skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_constituencies(parse):\n",
    "    constituencies = []\n",
    "    tokens = []\n",
    "    read_constituency(parse, 0, constituencies, tokens, 0)\n",
    "    return constituencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject(parse):\n",
    "    constituencies = read_constituencies(parse)\n",
    "    np_constituencies = [c for c in constituencies if c.tag == \"NP\"]\n",
    "    if len(np_constituencies) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        top_np_constituency = sorted(np_constituencies, key=lambda c: c.depth)[0]\n",
    "        return top_np_constituency.startIndex, top_np_constituency.endIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_resolution_map(corefs):\n",
    "    resolution_map = {}\n",
    "\n",
    "    for _, coref_group in corefs.items():\n",
    "        representative_mention = None\n",
    "        keys = []\n",
    "        for coref in coref_group:\n",
    "            if coref[\"isRepresentativeMention\"]:\n",
    "                representative_mention = coref[\"text\"]\n",
    "            keys.append((coref[\"sentNum\"], coref[\"startIndex\"], coref[\"endIndex\"]))\n",
    "        for key in keys:\n",
    "            resolution_map[key] = representative_mention    \n",
    "            \n",
    "    return resolution_map\n",
    "\n",
    "\n",
    "def resolve_subject(sentence_num, subject, resolution_map, tokens):\n",
    "    key = (sentence_num, *subject)\n",
    "    if key in resolution_map:\n",
    "        return resolution_map[key]\n",
    "    else:\n",
    "        return \" \".join(\n",
    "            [tokens[index - 1][\"originalText\"] for index in range(*subject)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resolved_subjects(text, nlp=nlp):\n",
    "    result = nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse,coref',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    resolution_map = create_resolution_map(result[\"corefs\"])\n",
    "    \n",
    "    resolved_subjects = []\n",
    "    for i, sent in enumerate(result[\"sentences\"]):\n",
    "        subject = get_subject(sent[\"parse\"])\n",
    "        if subject is None:\n",
    "            resolved_subject = None\n",
    "        else:\n",
    "            resolved_subject = resolve_subject(\n",
    "                i + 1,\n",
    "                subject,\n",
    "                resolution_map,\n",
    "                result[\"sentences\"][i][\"tokens\"]\n",
    "            )\n",
    "        resolved_subjects.append(resolved_subject)\n",
    "    \n",
    "    return resolved_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test__get_resolved_subjects():\n",
    "    text = \"This movie was actually neither that funny, nor super witty. The movie was meh. I liked watching that movie. If I had a choice, I would not watch that movie again.\"\n",
    "    assert get_resolved_subjects(text, nlp) == [\"This movie\", \"This movie\", \"I\", \"I\"]\n",
    "    \n",
    "\n",
    "test__get_resolved_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include previous sentences to do the coref resolution\n",
    "def move_st(text, st, n_prev_sentences):\n",
    "    sent_counter = 0\n",
    "    while st > 0:\n",
    "        if text[st] in {\".\", \"!\", \"?\"}:\n",
    "            sent_counter += 1\n",
    "            if sent_counter > n_prev_sentences:\n",
    "                return st + 1, sent_counter - 1\n",
    "        st -= 1\n",
    "    return 0, sent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_subject(text, st, end):\n",
    "    new_st, sent_no = move_st(text, st, 5)\n",
    "    resolved_subjects = get_resolved_subjects(text[new_st:end])\n",
    "    if len(resolved_subjects) > 0:\n",
    "        return resolved_subjects[-1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def test__find_subject():\n",
    "    assert find_subject(\n",
    "               \"Sam likes pizza. So he eats it frequently.\",\n",
    "               17, \n",
    "               42\n",
    "           ) == \"Sam\"\n",
    "    assert find_subject(\n",
    "               \"Sam likes pizza. So he eats it frequently.\",\n",
    "               0, \n",
    "               17\n",
    "           ) == \"Sam\"\n",
    "\n",
    "    \n",
    "test__find_subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_tokens(sent, offset=1):\n",
    "    output_tokens = []\n",
    "    for token in [t[\"word\"] for t in sent[\"tokens\"][(offset - 1):]]:\n",
    "        if token not in {\".\", \",\", \"?\", \"!\", \";\", \":\"}:\n",
    "            output_tokens.append(\" \")\n",
    "        output_tokens.append(token)\n",
    "    if len(output_tokens) > 0 and output_tokens[0] == \" \":\n",
    "        output_tokens = output_tokens[1:]\n",
    "    return \"\".join(output_tokens)           \n",
    "\n",
    "\n",
    "def remove_leading_words(text, verbose=False):\n",
    "    result = nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if len(result[\"sentences\"]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        sent = result[\"sentences\"][0]\n",
    "        constituencies = read_constituencies(sent[\"parse\"])\n",
    "        np_constituencies = [c for c in constituencies if c.tag == \"NP\"]\n",
    "        if len(np_constituencies) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            first_np_constituency = sorted(np_constituencies, key=lambda c: c.startIndex)[0]\n",
    "            new_text = \" \".join(\n",
    "                [join_tokens(sent, first_np_constituency.startIndex)]\n",
    "                + [join_tokens(s) for s in result[\"sentences\"][1:]]\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\"Removing tokens before the first NP:\")\n",
    "                print(\"-- syntactic parsing result\\n\", sent[\"parse\"])\n",
    "                print(f\"{text} \\n---> \\n{new_text}\")\n",
    "            return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_verb(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def is_plural(word):\n",
    "    if word == en.singularize(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "class Tense(enum.Enum):\n",
    "    PRESENT = 0\n",
    "    PAST = 1\n",
    "\n",
    "    \n",
    "class Number(enum.Enum):\n",
    "    SINGULAR = 0\n",
    "    PLURAL = 1\n",
    "    \n",
    "    \n",
    "def conjugate(verb, tense=Tense.PAST, person=1, number=Number.SINGULAR):\n",
    "    if tense == Tense.PRESENT:\n",
    "        en_tense = en.PRESENT\n",
    "    elif tense == Tense.PAST:\n",
    "        en_tense = en.PAST\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    if number == Number.SINGULAR:\n",
    "        en_number = en.SINGULAR\n",
    "    elif number == Number.PLURAL:\n",
    "        en_number = en.PLURAL\n",
    "    else: \n",
    "        assert False\n",
    "        \n",
    "    return en.conjugate(verb, tense=en_tense, person=person, number=en_number)\n",
    "\n",
    "\n",
    "def find_first_verb(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag[:1] == 'V':\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_first_adjective(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag == 'JJ':\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_parse_trees(text):\n",
    "    result = nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for i, sentence in enumerate(result[\"sentences\"]):\n",
    "        print(f\"SENTENCE no. {i}:\\n\")\n",
    "        print(sentence[\"parse\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Code for checking if the verbs of the parts of a relation belong to the same subject\n",
    "\n",
    "def print_if_verbose(text, verbose):\n",
    "    if verbose:\n",
    "        print(text)\n",
    "\n",
    "\n",
    "def make_constituencies_df(constituencies):\n",
    "    return pd.DataFrame(\n",
    "        [(c.tag, c.startIndex - 1, c.endIndex - 1, c.depth) for c in constituencies],\n",
    "        columns=[\"type\", \"start\", \"end\", \"depth\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def count_tokens(text, st, end):\n",
    "    return len(\n",
    "        nltk.tokenize.word_tokenize(text[st:end])\n",
    "    )\n",
    "\n",
    "\n",
    "def find_boundaries(text, left_st, left_end, right_st, right_end, verbose=False):\n",
    "    sentence_st, _ = move_st(text, left_st, 0)\n",
    "    print_if_verbose(f\"Sentence starts at {sentence_st}.\", verbose)\n",
    "    \n",
    "    first_left_token_no = count_tokens(text, sentence_st, left_st)  \n",
    "    last_left_token_no  = (\n",
    "        first_left_token_no + count_tokens(text, left_st, left_end)\n",
    "    )\n",
    "    first_right_token_no = (\n",
    "        last_left_token_no + count_tokens(text, left_end, right_st)\n",
    "    )\n",
    "    last_right_token_no = (\n",
    "        first_right_token_no + count_tokens(text, right_st, right_end)\n",
    "    )\n",
    "    return first_left_token_no, last_left_token_no, first_right_token_no, last_right_token_no\n",
    "\n",
    "\n",
    "def get_vps_in_boundaries(constituencies_df, left_boundary, right_boundary):\n",
    "    return constituencies_df.loc[\n",
    "        (constituencies_df.type == \"VP\")\n",
    "            & (\n",
    "                (constituencies_df.start >= left_boundary)\n",
    "                    & (constituencies_df.end <= right_boundary)\n",
    "            )\n",
    "    ]\n",
    "\n",
    "\n",
    "def find_vp_boundaries(constituencies_df, left_boundary, right_boundary):\n",
    "    vp_df = get_vps_in_boundaries(constituencies_df, left_boundary, right_boundary) \n",
    "    if len(vp_df) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        min_depth = vp_df.depth.min()\n",
    "        vp = vp_df.loc[vp_df.depth == min_depth].iloc[0]\n",
    "        return vp.start, vp.end\n",
    "\n",
    "    \n",
    "def find_umbrella_vps(constituencies_df, left_vp_boundaries, right_vp_boundaries):\n",
    "    return constituencies_df.loc[\n",
    "        (constituencies_df.type == \"VP\")\n",
    "            & (\n",
    "                (constituencies_df.start <= left_vp_boundaries[0])\n",
    "                    & (constituencies_df.end >= right_vp_boundaries[1])\n",
    "            )\n",
    "    ]\n",
    "\n",
    "\n",
    "def belong_to_one_vp(text, left_st, left_end, right_st, right_end, verbose=False):\n",
    "    st, _ = move_st(text, left_st, 0)\n",
    "    result = nlp.annotate(\n",
    "        text[st:],\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    if (\n",
    "        \"sentences\" not in result \n",
    "            or len(result[\"sentences\"]) == 0 \n",
    "            or \"parse\" not in result[\"sentences\"][0]\n",
    "    ):\n",
    "        print_if_verbose(\"Failed to parse the sentence.\", verbose)\n",
    "        return False\n",
    "    else:\n",
    "        print_if_verbose(f\"Parsing result:\\n{result['sentences'][0]['parse']}\\n\", verbose)\n",
    "            \n",
    "    constituencies_df = make_constituencies_df(\n",
    "        read_constituencies(result[\"sentences\"][0][\"parse\"])\n",
    "    )\n",
    "    print_if_verbose(f\"Constituencies:\\n{constituencies_df}\\n\", verbose)\n",
    "        \n",
    "    boundaries = find_boundaries(\n",
    "        text, left_st, left_end, right_st, right_end, verbose\n",
    "    )\n",
    "    print_if_verbose(f\"Boundaries: {boundaries}\", verbose)\n",
    "        \n",
    "    left_vp_boundaries = find_vp_boundaries(\n",
    "        constituencies_df, boundaries[0], boundaries[1]\n",
    "    )\n",
    "    if left_vp_boundaries is None:\n",
    "        print_if_verbose(\"Didn't find the left VP.\", verbose)\n",
    "        return False\n",
    "    \n",
    "    right_vp_boundaries = find_vp_boundaries(\n",
    "        constituencies_df, boundaries[2], boundaries[3]\n",
    "    )\n",
    "    if right_vp_boundaries is None:\n",
    "        print_if_verbose(\"Didn't find the right VP.\", verbose)\n",
    "        return False\n",
    "    print_if_verbose(\n",
    "        f\"Left and right VP boundaries: {left_vp_boundaries}, {right_vp_boundaries}\", verbose\n",
    "    )\n",
    "        \n",
    "    umbrella_vps = find_umbrella_vps(\n",
    "        constituencies_df, left_vp_boundaries, right_vp_boundaries\n",
    "    )\n",
    "    print_if_verbose(f\"Umbrella VPs:\\n{umbrella_vps}\", verbose)\n",
    "              \n",
    "    return len(umbrella_vps) > 0\n",
    "\n",
    "\n",
    "def test__belong_to_one_vp():\n",
    "    text = (\n",
    "        \"I saw Beijing and climbed the Great Wall. \"\n",
    "        \"This is our last chance to swim in the ocean and enjoy the warm weather.\"\n",
    "    )\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"I\"),\n",
    "        text.index(\"Beijing\") + 1,\n",
    "        text.index(\"and\"),\n",
    "        text.index(\"Wall\") + 1,\n",
    "        False\n",
    "    ) == True\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"to\"),\n",
    "        text.index(\"ocean\") + 1,\n",
    "        text.index(\"ocean\") + 1,\n",
    "        text.index(\"weather\") + 1,\n",
    "        False\n",
    "    ) == True\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"I\"),\n",
    "        text.index(\"Beijing\") + 1,\n",
    "        text.index(\"This\"),\n",
    "        text.index(\"weather\") + 1,\n",
    "        False\n",
    "    ) == False\n",
    "    \n",
    "    \n",
    "test__belong_to_one_vp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
