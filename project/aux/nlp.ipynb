{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pattern.en as en\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford_nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Constituency = namedtuple(\n",
    "    \"Constituency\", [\"tag\", \"startIndex\", \"endIndex\", \"depth\", \"parent_tag\"]\n",
    ")\n",
    "\n",
    "\n",
    "def skip_blank_space(parse, pointer):\n",
    "    while pointer < len(parse) and parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return pointer\n",
    "\n",
    "\n",
    "def read_tag(parse, pointer):\n",
    "    tag_start = pointer\n",
    "    while pointer < len(parse) and not parse[pointer].isspace():\n",
    "        pointer += 1\n",
    "    return parse[tag_start:pointer], skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_token(parse, pointer, tokens):\n",
    "    token_start = pointer\n",
    "    while pointer < len(parse) and parse[pointer] != \")\":\n",
    "        pointer += 1\n",
    "    tokens.append(parse[token_start:pointer])\n",
    "    return pointer\n",
    "    \n",
    "    \n",
    "def read_body(parse, pointer, constituencies, tokens, depth, parent_tag):\n",
    "    if parse[pointer] == \"(\":\n",
    "        return read_constituency(parse, pointer, constituencies, tokens, depth, parent_tag)\n",
    "    else:\n",
    "        return None, read_token(parse, pointer, tokens)\n",
    "        \n",
    "        \n",
    "def read_constituency(parse, pointer, constituencies, tokens, depth, parent_tag):\n",
    "    assert parse[pointer] == \"(\"\n",
    "    pointer += 1\n",
    "    tag, pointer = read_tag(parse, pointer)\n",
    "    first_child, pointer = read_body(\n",
    "        parse, pointer, constituencies, tokens, depth + 1, parent_tag=tag\n",
    "    )\n",
    "    if first_child is None:\n",
    "        constituency = Constituency(\n",
    "            tag=tag, \n",
    "            startIndex=len(tokens), \n",
    "            endIndex=len(tokens) + 1, \n",
    "            depth=depth,\n",
    "            parent_tag=parent_tag\n",
    "        )\n",
    "        assert parse[pointer] == \")\"\n",
    "    else:\n",
    "        child = first_child\n",
    "        while parse[pointer] != \")\":\n",
    "            child, pointer = read_body(\n",
    "                parse, pointer, constituencies, tokens, depth + 1, parent_tag=tag\n",
    "            )\n",
    "        constituency = Constituency(\n",
    "            tag=tag, \n",
    "            startIndex=first_child.startIndex, \n",
    "            endIndex=child.endIndex, \n",
    "            depth=depth,\n",
    "            parent_tag=parent_tag\n",
    "        )\n",
    "    pointer += 1\n",
    "    constituencies.append(constituency)  \n",
    "    return constituency, skip_blank_space(parse, pointer)\n",
    "\n",
    "\n",
    "def read_constituencies(parse):\n",
    "    constituencies = []\n",
    "    tokens = []\n",
    "    read_constituency(parse, 0, constituencies, tokens, 0, \"\")\n",
    "    return constituencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject(parse):\n",
    "    constituencies = read_constituencies(parse)\n",
    "    np_constituencies = [c for c in constituencies if c.tag == \"NP\"]\n",
    "    if len(np_constituencies) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        top_np_constituency = sorted(np_constituencies, key=lambda c: c.depth)[0]\n",
    "        return top_np_constituency.startIndex, top_np_constituency.endIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_resolution_map(corefs):\n",
    "    resolution_map = {}\n",
    "\n",
    "    for _, coref_group in corefs.items():\n",
    "        representative_mention = None\n",
    "        keys = []\n",
    "        for coref in coref_group:\n",
    "            if coref[\"isRepresentativeMention\"]:\n",
    "                representative_mention = coref[\"text\"]\n",
    "            keys.append((coref[\"sentNum\"], coref[\"startIndex\"], coref[\"endIndex\"]))\n",
    "        for key in keys:\n",
    "            resolution_map[key] = representative_mention    \n",
    "            \n",
    "    return resolution_map\n",
    "\n",
    "\n",
    "def resolve_subject(sentence_num, subject, resolution_map, tokens):\n",
    "    key = (sentence_num, *subject)\n",
    "    if key in resolution_map:\n",
    "        return resolution_map[key]\n",
    "    else:\n",
    "        return \" \".join(\n",
    "            [tokens[index - 1][\"originalText\"] for index in range(*subject)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resolved_subjects(text, stanford_nlp=stanford_nlp):\n",
    "    result = stanford_nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse,coref',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    resolution_map = create_resolution_map(result[\"corefs\"])\n",
    "    \n",
    "    resolved_subjects = []\n",
    "    for i, sent in enumerate(result[\"sentences\"]):\n",
    "        subject = get_subject(sent[\"parse\"])\n",
    "        if subject is None:\n",
    "            resolved_subject = None\n",
    "        else:\n",
    "            resolved_subject = resolve_subject(\n",
    "                i + 1,\n",
    "                subject,\n",
    "                resolution_map,\n",
    "                result[\"sentences\"][i][\"tokens\"]\n",
    "            )\n",
    "        resolved_subjects.append(resolved_subject)\n",
    "    \n",
    "    return resolved_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test__get_resolved_subjects():\n",
    "    text = \"This movie was actually neither that funny, nor super witty. The movie was meh. I liked watching that movie. If I had a choice, I would not watch that movie again.\"\n",
    "    assert get_resolved_subjects(text, stanford_nlp) == [\"This movie\", \"This movie\", \"I\", \"I\"]\n",
    "    \n",
    "\n",
    "test__get_resolved_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include previous sentences to do the coref resolution\n",
    "def move_st(text, st, n_prev_sentences):\n",
    "    sent_counter = 0\n",
    "    while st > 0:\n",
    "        if text[st] in {\".\", \"!\", \"?\"}:\n",
    "            sent_counter += 1\n",
    "            if sent_counter > n_prev_sentences:\n",
    "                return st + 1, sent_counter - 1\n",
    "        st -= 1\n",
    "    return 0, sent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_subject(text, st, end):\n",
    "    new_st, sent_no = move_st(text, st, 5)\n",
    "    resolved_subjects = get_resolved_subjects(text[new_st:end])\n",
    "    if len(resolved_subjects) > 0:\n",
    "        return resolved_subjects[-1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def test__find_subject():\n",
    "    assert find_subject(\n",
    "               \"Sam likes pizza. So he eats it frequently.\",\n",
    "               17, \n",
    "               42\n",
    "           ) == \"Sam\"\n",
    "    assert find_subject(\n",
    "               \"Sam likes pizza. So he eats it frequently.\",\n",
    "               0, \n",
    "               17\n",
    "           ) == \"Sam\"\n",
    "\n",
    "    \n",
    "test__find_subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_verb(verb):\n",
    "    return en.conjugate(\n",
    "        verb, \n",
    "        tense = en.PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "        person = 1,              # 1, 2, 3 or None\n",
    "        number = en.SINGULAR,       # SG, PL\n",
    "        mood = en.INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "        negated = False,          # True or False\n",
    "        parse = True\n",
    "    )\n",
    "\n",
    "\n",
    "def is_plural(word):\n",
    "    if word == en.singularize(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "class Tense(enum.Enum):\n",
    "    PRESENT = 0\n",
    "    PAST = 1\n",
    "\n",
    "    \n",
    "class Number(enum.Enum):\n",
    "    SINGULAR = 0\n",
    "    PLURAL = 1\n",
    "    \n",
    "    \n",
    "def conjugate(verb, tense=Tense.PAST, person=1, number=Number.SINGULAR):\n",
    "    if tense == Tense.PRESENT:\n",
    "        en_tense = en.PRESENT\n",
    "    elif tense == Tense.PAST:\n",
    "        en_tense = en.PAST\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    if number == Number.SINGULAR:\n",
    "        en_number = en.SINGULAR\n",
    "    elif number == Number.PLURAL:\n",
    "        en_number = en.PLURAL\n",
    "    else: \n",
    "        assert False\n",
    "        \n",
    "    return en.conjugate(verb, tense=en_tense, person=person, number=en_number)\n",
    "\n",
    "\n",
    "def find_first_verb(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag[:1] == 'V':\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_first_adjective(text):\n",
    "    for i, (_, pos_tag) in enumerate(en.tag(text)):\n",
    "        if pos_tag == 'JJ':\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_odd_expressions = {\n",
    "    tuple(nltk.tokenize.word_tokenize(odd_expression))\n",
    "        for odd_expression in [\n",
    "            \"in fact\",\n",
    "            \"as a matter of fact\",\n",
    "            \"actually\", \n",
    "            \"indeed\", \n",
    "            \"also\", \n",
    "            \"besides\", \n",
    "            \"too\", \n",
    "            \"as well\",\n",
    "            \"in particular\", \n",
    "            \"first of all\", \n",
    "            \"secondly\", \n",
    "            \"finally\", \n",
    "            \"to sum up\", \n",
    "            \"in conclusion\", \n",
    "            \"briefly\", \n",
    "            \"in short\", \n",
    "            \"on the whole\", \n",
    "            \"in general\", \n",
    "            \"in some cases\", \n",
    "            \"to some extent\", \n",
    "            \"broadly speaking\", \n",
    "            \"as a result\", \n",
    "            \"despite that\", \n",
    "            \"in spite of that\", \n",
    "            \"however\", \n",
    "            \"on the other hand\", \n",
    "            \"after all\", \n",
    "            \"on the contrary\", \n",
    "            \"of course\", \n",
    "            \"certainly\", \n",
    "            \"moreover\", \n",
    "            \"furthermore\", \n",
    "            \"further\", \n",
    "            \"in addition\", \n",
    "            \"also\", \n",
    "            \"besides\", \n",
    "            \"what is more\", \n",
    "            \"for instance\", \n",
    "            \"for example\", \n",
    "            \"e.g.\", \n",
    "            \"honestly\", \n",
    "            \"no doubt\", \n",
    "            \"that is to say\", \n",
    "            \"in other words\", \n",
    "            \"apparantly\", \n",
    "            \"really\", \n",
    "            \"actually\", \n",
    "            \"more or less\", \n",
    "            \"at least\", \n",
    "            \"in any case\", \n",
    "            \"in fact\", \n",
    "            \"already\"\n",
    "        ]\n",
    "}\n",
    "max_odd_expr_len = max([len(toe) for toe in tokenized_odd_expressions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_and_join_tokens(sent, offset=1, resolution_map=None, verbose=False):\n",
    "    output_words = []\n",
    "    prev_word = \"\"\n",
    "    i = offset - 1\n",
    "    while i < len(sent[\"tokens\"]):\n",
    "        \n",
    "        skipped_tokens = False\n",
    "        \n",
    "        k = max_odd_expr_len\n",
    "        while not skipped_tokens and k >= 1:\n",
    "            k_gram = tuple([t[\"word\"].lower() for t in sent[\"tokens\"][i:(i + k)]])\n",
    "            if k_gram in tokenized_odd_expressions:\n",
    "                if prev_word == \",\":\n",
    "                    assert len(output_words) >= 1\n",
    "                    output_words.pop()\n",
    "                    if len(output_words) > 0:\n",
    "                        prev_word = output_words[-1]\n",
    "                    else:\n",
    "                        prev_word = \"\"\n",
    "                if verbose:\n",
    "                    print(f\"Removing {' '.join(k_gram)}\")\n",
    "                i += k\n",
    "                skipped_tokens = True\n",
    "                if i < len(sent[\"tokens\"]) and sent[\"tokens\"][i][\"word\"] == \",\":\n",
    "                    i += 1\n",
    "            k -= 1\n",
    "                    \n",
    "        if not skipped_tokens:        \n",
    "            token = sent[\"tokens\"][i]\n",
    "            word = token[\"word\"]\n",
    "            if word == \"-LRB-\":\n",
    "                word = \"(\"\n",
    "            elif word == \"-RRB-\":\n",
    "                word = \")\"\n",
    "            if (\n",
    "                word not in {\".\", \",\", \"?\", \"!\", \";\", \":\", \")\"}\n",
    "                    and word[:1] != \"'\"\n",
    "                    and prev_word != \"(\"\n",
    "            ):\n",
    "                output_words.append(\" \")\n",
    "            if resolution_map is None:\n",
    "                output_words.append(word)\n",
    "            else:\n",
    "                if word.lower() in {\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"them\"}:\n",
    "                    key = (sent[\"index\"] + 1, token[\"index\"], token[\"index\"] + 1)\n",
    "                    if not key in resolution_map:\n",
    "                        output_words.append(word)\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(\n",
    "                                f\"Substituting '{resolution_map[key]}'\"\n",
    "                                f\" for '{token['word']}'.\"\n",
    "                            )\n",
    "                        output_words.append(resolution_map[key])\n",
    "                else:\n",
    "                    output_words.append(word)\n",
    "            prev_word = word\n",
    "            i += 1\n",
    "            \n",
    "    if len(output_words) > 0 and output_words[0] == \" \":\n",
    "        output_words = output_words[1:]\n",
    "    return \"\".join(output_words)           \n",
    "\n",
    "\n",
    "def test__process_and_join_tokens():\n",
    "    def apply_to_sentence(sentence, offset=1, resolution_map=None):\n",
    "        result = stanford_nlp.annotate(\n",
    "            sentence,\n",
    "            properties={\n",
    "               'annotators': 'parse',\n",
    "               'outputFormat': 'json',\n",
    "               'timeout': 60000,\n",
    "            }\n",
    "        )\n",
    "        return process_and_join_tokens(\n",
    "            result[\"sentences\"][0], offset, resolution_map\n",
    "        )\n",
    "\n",
    "    assert ( # Nothing is removed when there're no odd expressions\n",
    "        apply_to_sentence(\"We, them, and you all got a solution.\")\n",
    "            == \"We, them, and you all got a solution.\"\n",
    "    )\n",
    "    assert ( # Correctly handles several odd expressions following each other\n",
    "             # Correctly handles commas\n",
    "        apply_to_sentence(\"As a matter of fact, at least, we got a solution.\")\n",
    "            == \"we got a solution.\"\n",
    "    )\n",
    "    assert ( # Correctly removes odd expr. at the end of the sentence\n",
    "             # Correctly handles commas\n",
    "        apply_to_sentence(\"We got a solution, at least\")\n",
    "            == \"We got a solution\"\n",
    "    )\n",
    "    assert ( # Correctly removes odd expr. in the middle of the sentence\n",
    "             # Correctly handles commas\n",
    "        apply_to_sentence(\"We, at least, got a solution.\")\n",
    "            == \"We got a solution.\"\n",
    "    )\n",
    "    assert ( # Correctly removes odd expr. at the end of the sentence\n",
    "             # Correctly handles the no-comma situation\n",
    "        apply_to_sentence(\"We got a solution too\")\n",
    "            == \"We got a solution\"\n",
    "    )\n",
    "    assert ( # Correctly removes odd expr. in the middle of the sentence\n",
    "             # Correctly handles the no-comma situation\n",
    "        apply_to_sentence(\"We also got a solution.\")\n",
    "            == \"We got a solution.\"\n",
    "    )\n",
    "    assert ( \n",
    "        apply_to_sentence(\"( 4 ) As a matter of fact, at least, we got a solution.\")\n",
    "            == \"(4) we got a solution.\"\n",
    "    )\n",
    "    assert ( \n",
    "        apply_to_sentence(\"ABCDE He really wanted to go.\", 2, {(1, 2, 3): \"Sam\"})\n",
    "            == \"Sam wanted to go.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "test__process_and_join_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_and_fix_punctuation(sent):\n",
    "    trimmed = sent.strip()\n",
    "    if len(trimmed) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        if trimmed[-1] in {\",\", \";\", \":\"}:\n",
    "            return trimmed[:-1] + \".\"\n",
    "        elif trimmed[-1] not in {\"!\", \"?\", \".\"}:\n",
    "            return trimmed + \".\"\n",
    "        else:\n",
    "            return trimmed\n",
    "    \n",
    "\n",
    "def is_imperative_verb(token):\n",
    "    if len(token[\"pos\"]) == 0 or token[\"pos\"][0] != \"V\":\n",
    "        return False\n",
    "    else:\n",
    "        return normalise_verb(token[\"word\"]) == token[\"word\"].lower()\n",
    "    \n",
    "    \n",
    "def find_comma_index(sent):\n",
    "    for token in sent[\"tokens\"]:\n",
    "        if token[\"word\"] == \",\":\n",
    "            return token[\"index\"]\n",
    "    return -1\n",
    "\n",
    " \n",
    "def get_offset(sent, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"-- syntactic parsing result\\n\", sent[\"parse\"])\n",
    "    if (\n",
    "        sent[\"tokens\"][0][\"word\"].lower() in {\"if\", \"when\", \"as\"}\n",
    "            or is_imperative_verb(sent[\"tokens\"][0])\n",
    "    ):\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"No tokens before the first NP will be removed because\"\n",
    "                \" the sentence starts with if/when/as or an imperative verb.\"\n",
    "            )\n",
    "        return 1\n",
    "    else:\n",
    "        first_np = None\n",
    "#             comma_index = find_comma_index(sent)\n",
    "        constituencies = read_constituencies(sent[\"parse\"])\n",
    "        for c in constituencies:\n",
    "            if c.tag == \"NP\" and c.parent_tag == \"S\": # and c.startIndex > comma_index:\n",
    "                if first_np is None or first_np.startIndex > c.startIndex:\n",
    "                    first_np = c\n",
    "        if first_np is None:\n",
    "            return 1\n",
    "        else:\n",
    "            return first_np.startIndex\n",
    "\n",
    "\n",
    "def take_first_sentence_and_remove_leading_words(text, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Taking the first sentence and removing leading words:\")\n",
    "    \n",
    "    result = stanford_nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if len(result[\"sentences\"]) == 0 or len(result[\"sentences\"][0]) == 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"The parser didn't extract any sentences\"\n",
    "                \" or the length of the first sentence is zero.\"\n",
    "            )\n",
    "        return None\n",
    "    else:\n",
    "        sent = result[\"sentences\"][0]\n",
    "        offset = get_offset(sent, verbose)\n",
    "        new_text = trim_and_fix_punctuation(\n",
    "            \" \".join(\n",
    "                [process_and_join_tokens(sent, offset)]\n",
    "#                     + [join_tokens(s) for s in result[\"sentences\"][1:]]\n",
    "            )\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"{text} \\n---> \\n{new_text}\")\n",
    "        return new_text\n",
    "\n",
    "        \n",
    "def test__take_first_sentence_and_remove_leading_words():\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"In fact, we expected it. We did!\") \n",
    "            == \"we expected it.\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"In fact we expected it.  \") \n",
    "            == \"we expected it.\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"Do what I say! Now!\") \n",
    "            == \"Do what I say!\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"If so, let's skip it ( 4 ) :\") \n",
    "            == \"If so, let's skip it (4).\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"When in doubt, ask them, \") \n",
    "            == \"When in doubt, ask them.\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"As agreed, we'll write it down.\") \n",
    "            == \"As agreed, we'll write it down.\"\n",
    "    )\n",
    "    assert (\n",
    "        take_first_sentence_and_remove_leading_words(\"At last they went to Spain, \") \n",
    "            == \"they went to Spain.\"\n",
    "    )\n",
    "    assert take_first_sentence_and_remove_leading_words(\"\") is None\n",
    "\n",
    "\n",
    "test__take_first_sentence_and_remove_leading_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_last_sentence_and_resolve_pronouns(text, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Taking the last sentence and resolving pronouns:\")\n",
    "    \n",
    "    st, _ = move_st(text, len(text) - 1, 4)\n",
    "    result = stanford_nlp.annotate(\n",
    "        text[-st:],\n",
    "        properties={\n",
    "           'annotators': 'parse,coref',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 100000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    resolution_map = create_resolution_map(result[\"corefs\"])\n",
    "    \n",
    "    if len(result[\"sentences\"]) == 0 or len(result[\"sentences\"][-1]) == 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"The parser didn't extract any sentences\"\n",
    "                \" or the length of the last sentence is zero.\"\n",
    "            )\n",
    "        return None\n",
    "    else:\n",
    "        sent = result[\"sentences\"][-1]\n",
    "        offset = get_offset(sent)\n",
    "        new_text = trim_and_fix_punctuation(\n",
    "            process_and_join_tokens(sent, offset, resolution_map, verbose)\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"{text} \\n---> \\n{new_text}\")\n",
    "        return new_text\n",
    "    \n",
    "    \n",
    "def test__take_last_sentence_and_resolve_pronouns():\n",
    "    assert (\n",
    "        take_last_sentence_and_resolve_pronouns(\n",
    "            \"Sam likes icecream. He eats it everyday.\"\n",
    "        )\n",
    "    ) == \"Sam eats icecream everyday.\"\n",
    "\n",
    "    assert (\n",
    "        take_last_sentence_and_resolve_pronouns(\n",
    "            \"Sam likes icecream. See (5 )\"\n",
    "        )\n",
    "    ) == \"See (5).\"\n",
    "    \n",
    "    assert take_last_sentence_and_resolve_pronouns(\"\") is None\n",
    "    \n",
    "    assert (\n",
    "        take_last_sentence_and_resolve_pronouns(\n",
    "            \"Sam likes icecream. But he does not eat it everyday.\"\n",
    "        )\n",
    "    ) == \"Sam does not eat icecream everyday.\"\n",
    "    \n",
    "    \n",
    "test__take_last_sentence_and_resolve_pronouns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_parse_trees(text):\n",
    "    result = stanford_nlp.annotate(\n",
    "        text,\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for i, sentence in enumerate(result[\"sentences\"]):\n",
    "        print(f\"SENTENCE no. {i}:\\n\")\n",
    "        print(sentence[\"parse\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Code for checking if the verbs of the parts of a relation belong to the same subject\n",
    "\n",
    "def print_if_verbose(text, verbose):\n",
    "    if verbose:\n",
    "        print(text)\n",
    "\n",
    "\n",
    "def make_constituencies_df(constituencies):\n",
    "    return pd.DataFrame(\n",
    "        [(c.tag, c.startIndex - 1, c.endIndex - 1, c.depth) for c in constituencies],\n",
    "        columns=[\"type\", \"start\", \"end\", \"depth\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def count_tokens(text, st, end):\n",
    "    result = stanford_nlp.annotate(\n",
    "        text[st:end],\n",
    "        properties={\n",
    "           'annotators': 'tokenize',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    return len(result[\"tokens\"])\n",
    "# def count_tokens(text, st, end):\n",
    "#     return len(\n",
    "#         nltk.tokenize.word_tokenize(text[st:end])\n",
    "#     )\n",
    "\n",
    "\n",
    "def find_boundaries(text, left_st, left_end, right_st, right_end, verbose=False):\n",
    "    sentence_st, _ = move_st(text, left_st, 0)\n",
    "    print_if_verbose(f\"Sentence starts at {sentence_st}.\", verbose)\n",
    "    \n",
    "    first_left_token_no = count_tokens(text, sentence_st, left_st)  \n",
    "    last_left_token_no  = (\n",
    "        first_left_token_no + count_tokens(text, left_st, left_end)\n",
    "    )\n",
    "    first_right_token_no = (\n",
    "        last_left_token_no + count_tokens(text, left_end, right_st)\n",
    "    )\n",
    "    last_right_token_no = (\n",
    "        first_right_token_no + count_tokens(text, right_st, right_end)\n",
    "    )\n",
    "    return first_left_token_no, last_left_token_no, first_right_token_no, last_right_token_no\n",
    "\n",
    "\n",
    "def get_vps_in_boundaries(constituencies_df, left_boundary, right_boundary):\n",
    "    return constituencies_df.loc[\n",
    "        (constituencies_df.type == \"VP\")\n",
    "            & (\n",
    "                (constituencies_df.start >= left_boundary)\n",
    "                    & (constituencies_df.end <= right_boundary)\n",
    "            )\n",
    "    ]\n",
    "\n",
    "\n",
    "def find_vp_boundaries(constituencies_df, left_boundary, right_boundary):\n",
    "    vp_df = get_vps_in_boundaries(constituencies_df, left_boundary, right_boundary) \n",
    "    if len(vp_df) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        min_depth = vp_df.depth.min()\n",
    "        vp = vp_df.loc[vp_df.depth == min_depth].iloc[0]\n",
    "        return vp.start, vp.end\n",
    "\n",
    "    \n",
    "def find_umbrella_vps(constituencies_df, left_vp_boundaries, right_vp_boundaries):\n",
    "    return constituencies_df.loc[\n",
    "        (constituencies_df.type == \"VP\")\n",
    "            & (\n",
    "                (constituencies_df.start <= left_vp_boundaries[0])\n",
    "                    & (constituencies_df.end >= right_vp_boundaries[1])\n",
    "            )\n",
    "    ]\n",
    "\n",
    "\n",
    "def belong_to_one_vp(text, left_st, left_end, right_st, right_end, verbose=False):\n",
    "    st, _ = move_st(text, left_st, 0)\n",
    "    result = stanford_nlp.annotate(\n",
    "        text[st:],\n",
    "        properties={\n",
    "           'annotators': 'parse',\n",
    "           'outputFormat': 'json',\n",
    "           'timeout': 60000,\n",
    "        }\n",
    "    )\n",
    "    if (\n",
    "        \"sentences\" not in result \n",
    "            or len(result[\"sentences\"]) == 0 \n",
    "            or \"parse\" not in result[\"sentences\"][0]\n",
    "    ):\n",
    "        print_if_verbose(\"Failed to parse the sentence.\", verbose)\n",
    "        return False\n",
    "    else:\n",
    "        print_if_verbose(f\"Parsing result:\\n{result['sentences'][0]['parse']}\\n\", verbose)\n",
    "            \n",
    "    constituencies_df = make_constituencies_df(\n",
    "        read_constituencies(result[\"sentences\"][0][\"parse\"])\n",
    "    )\n",
    "    print_if_verbose(f\"Constituencies:\\n{constituencies_df}\\n\", verbose)\n",
    "        \n",
    "    boundaries = find_boundaries(\n",
    "        text, left_st, left_end, right_st, right_end, verbose\n",
    "    )\n",
    "    print_if_verbose(f\"Boundaries: {boundaries}\", verbose)\n",
    "        \n",
    "    left_vp_boundaries = find_vp_boundaries(\n",
    "        constituencies_df, boundaries[0], boundaries[1]\n",
    "    )\n",
    "    if left_vp_boundaries is None:\n",
    "        print_if_verbose(\"Didn't find the left VP.\", verbose)\n",
    "        return False\n",
    "    \n",
    "    right_vp_boundaries = find_vp_boundaries(\n",
    "        constituencies_df, boundaries[2], boundaries[3]\n",
    "    )\n",
    "    if right_vp_boundaries is None:\n",
    "        print_if_verbose(\"Didn't find the right VP.\", verbose)\n",
    "        return False\n",
    "    print_if_verbose(\n",
    "        f\"Left and right VP boundaries: {left_vp_boundaries}, {right_vp_boundaries}\", verbose\n",
    "    )\n",
    "        \n",
    "    umbrella_vps = find_umbrella_vps(\n",
    "        constituencies_df, left_vp_boundaries, right_vp_boundaries\n",
    "    )\n",
    "    print_if_verbose(f\"Umbrella VPs:\\n{umbrella_vps}\", verbose)\n",
    "              \n",
    "    return len(umbrella_vps) > 0\n",
    "\n",
    "\n",
    "def test__belong_to_one_vp():\n",
    "    text = (\n",
    "        \"I saw Beijing and climbed the Great Wall. \"\n",
    "        \"This is our last chance to swim in the ocean and enjoy the warm weather.\"\n",
    "    )\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"I\"),\n",
    "        text.index(\"Beijing\") + 1,\n",
    "        text.index(\"and\"),\n",
    "        text.index(\"Wall\") + 1,\n",
    "        False\n",
    "    ) == True\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"to\"),\n",
    "        text.index(\"ocean\") + 1,\n",
    "        text.index(\"ocean\") + 1,\n",
    "        text.index(\"weather\") + 1,\n",
    "        False\n",
    "    ) == True\n",
    "\n",
    "    assert belong_to_one_vp(\n",
    "        text, \n",
    "        text.index(\"I\"),\n",
    "        text.index(\"Beijing\") + 1,\n",
    "        text.index(\"This\"),\n",
    "        text.index(\"weather\") + 1,\n",
    "        False\n",
    "    ) == False\n",
    "    \n",
    "    \n",
    "test__belong_to_one_vp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
