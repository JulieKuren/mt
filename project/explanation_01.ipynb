{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /Users/YK/mt/project/aux/utils.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/nlp.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/relation_extraction.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/defs.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/preparation.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from aux import utils\n",
    "from aux import nlp\n",
    "from aux import relation_extraction\n",
    "from aux import defs\n",
    "from aux import preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_subjects_differ(text, rel, n_prev_sent=5):\n",
    "    new_st, left_segment_sent_no = nlp.move_st(\n",
    "        text, rel.left.start, n_prev_sent\n",
    "    )\n",
    "    if len({\".\", \"!\", \"?\"}.intersection(\n",
    "        set(word_tokenize(text[rel.left.start:rel.right.end])))) > 0:\n",
    "        right_segment_sent_no = left_segment_sent_no + 1\n",
    "    else:\n",
    "        right_segment_sent_no = left_segment_sent_no\n",
    "\n",
    "    if left_segment_sent_no == right_segment_sent_no:\n",
    "        return True\n",
    "    else:\n",
    "        resolved_subjects = nlp.get_resolved_subjects(text[new_st:rel.right.end])\n",
    "        if len(resolved_subjects) < right_segment_sent_no:\n",
    "            return True\n",
    "        else:\n",
    "            return (\n",
    "                resolved_subjects[left_segment_sent_no] \n",
    "                != resolved_subjects[right_segment_sent_no]\n",
    "            )\n",
    "\n",
    "        \n",
    "def test__do_subjects_differ():\n",
    "    text = \"\"\"Nika lives in Berlin. She goes to a Kita. Her favourite color is yellow.\"\"\"\n",
    "    assert do_subjects_differ(\n",
    "        text, \n",
    "        relation_extraction.Relation(\n",
    "            \"\", \n",
    "            relation_extraction.Segment(\"N\", 22, 42), \n",
    "            relation_extraction.Segment(\"S\", 42, len(text)), \n",
    "            None, \n",
    "            None\n",
    "        )\n",
    "    )\n",
    "    text = \"\"\"Nika lives in Berlin. She goes to a Kita. She likes yellow things.\"\"\"\n",
    "    assert not do_subjects_differ(\n",
    "        text, \n",
    "        relation_extraction.Relation(\n",
    "            \"\", \n",
    "            relation_extraction.Segment(\"N\", 22, 42), \n",
    "            relation_extraction.Segment(\"S\", 42, len(text)), \n",
    "            None, \n",
    "            None\n",
    "        )\n",
    "    )    \n",
    "\n",
    "    \n",
    "test__do_subjects_differ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItMakesParseResult = collections.namedtuple(\n",
    "    \"ItMakesParseResult\", [\"success\", \"subject\", \"verb\", \"rest\"]\n",
    ")\n",
    "\n",
    "\n",
    "def safe_access(tokens, i):\n",
    "    if i < len(tokens):\n",
    "        return tokens[i]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "def normalise_and_identify_person(subject):\n",
    "    subject = subject.lower()\n",
    "    if subject == \"me\":\n",
    "        return \"I\", 1\n",
    "    elif subject == \"us\":\n",
    "        return \"we\", 1\n",
    "    elif subject == \"you\":\n",
    "        return \"you\", 2\n",
    "    elif subject == \"him\":\n",
    "        return \"he\", 3\n",
    "    elif subject == \"her\":\n",
    "        return \"she\", 3\n",
    "    elif subject == \"them\":\n",
    "        return \"they\", 3\n",
    "    else:\n",
    "        return subject, 3\n",
    "    \n",
    "    \n",
    "def parse_it_makes(tokens, verbose=False):\n",
    "    if safe_access(tokens, 0).lower() == \"it\":\n",
    "        make_pos = -1\n",
    "        \n",
    "        if nlp.normalise_verb(safe_access(tokens, 1).lower()) == \"make\": #it makes ...\n",
    "            make_pos = 1\n",
    "        elif nlp.normalise_verb(safe_access(tokens, 2).lower()) == \"make\": #it is making ...\n",
    "            make_pos = 2\n",
    "        if make_pos < 0:\n",
    "            utils.print_if_verbose(\"Didn't find 'make'.\", verbose)\n",
    "            return ItMakesParseResult(False, None, None, None)\n",
    "        else:\n",
    "            first_verb_position = nlp.find_first_verb(\" \".join(tokens[make_pos + 1:]))\n",
    "            if first_verb_position is None:\n",
    "                utils.print_if_verbose(\n",
    "                    \"Didn't find the verb. Will look for an adjective.\", verbose\n",
    "                ) #It makes me sad.\n",
    "                first_adj_position = nlp.find_first_adjective(\" \".join(tokens[make_pos + 1:]))\n",
    "                if first_adj_position is None:\n",
    "                    utils.print_if_verbose(\"Didn't find an adjective either.\")\n",
    "                    return ItMakesParseResult(False, None, None, None)\n",
    "                else:\n",
    "                    verb = \"be\" #I am sad\n",
    "                    subject = \" \".join(\n",
    "                        tokens[make_pos + 1:(make_pos + 1 + first_adj_position)]\n",
    "                    )\n",
    "                    rest = \" \".join(tokens[(make_pos + 1 + first_adj_position):])\n",
    "            else:\n",
    "                verb = tokens[make_pos + 1 + first_verb_position]\n",
    "                subject = \" \".join(\n",
    "                    tokens[make_pos + 1:(make_pos + 1 + first_verb_position)]\n",
    "                )\n",
    "                rest = \" \".join(tokens[(make_pos + 1 + first_verb_position + 1):])\n",
    "            \n",
    "            if tokens[make_pos] in {\"make\", \"makes\", \"making\"}:\n",
    "                tense = nlp.Tense.PRESENT\n",
    "            elif tokens[make_pos] == \"made\":\n",
    "                tense = nlp.Tense.PAST\n",
    "            else:\n",
    "                assert False\n",
    "            normalised_subject, person = normalise_and_identify_person(subject)\n",
    "            if nlp.is_plural(normalised_subject):\n",
    "                number = nlp.Number.PLURAL\n",
    "            else:\n",
    "                number = nlp.Number.SINGULAR\n",
    "            return ItMakesParseResult(\n",
    "                True, \n",
    "                normalised_subject, \n",
    "                nlp.conjugate(verb, tense, person, number),\n",
    "                rest\n",
    "            )\n",
    "    else:\n",
    "        return ItMakesParseResult(False, None, None, None)\n",
    "\n",
    "        \n",
    "def check_parse_result(parse_result, true_subject, true_verb):\n",
    "    return parse_result.subject == true_subject and parse_result.verb == true_verb\n",
    "\n",
    "\n",
    "def test__parse_it_makes():\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It's making him feel happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"he\", \"feels\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It makes them feel happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"they\", \"feel\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It makes the cats happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"the cats\", \"are\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It makes the cat happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"the cat\", \"is\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It has made the cat happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"the cat\", \"was\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It has made the cats happy.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"the cat\", \"were\")\n",
    "\n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It has made the cats go home.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"the cats\", \"went\")\n",
    "    \n",
    "    parse_result = parse_it_makes(\n",
    "        utils.lowercase_and_tokenize(\"It has made Mr. Smith go home.\")\n",
    "    )\n",
    "    check_parse_result(parse_result, \"Mr. Smith\", \"went\")\n",
    "\n",
    "    \n",
    "test__parse_it_makes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RuleExplanation01(defs.Rule):\n",
    "    name = \"explanation_01\"\n",
    "    relation_type = \"Explanation\"\n",
    "    reasons = {\n",
    "        \"N_STARTS_WITH_BUT\": \n",
    "            defs.Reason(1, \"Nucleus starts with 'but'.\"),\n",
    "        \"N_STARTS_WITH_IF\": \n",
    "            defs.Reason(\n",
    "                2, \n",
    "                \"Nucleus starts with 'if' and its left subrelation is not 'Condition'.\"\n",
    "            ),\n",
    "        \"N_CONTAINS_BECAUSE\": \n",
    "            defs.Reason(\n",
    "                3, \n",
    "                \"Nucleus contains 'because'.\"\n",
    "            ),\n",
    "        \"SN_STARTS_WITH_BUT\": \n",
    "            defs.Reason(4, \"Satellite's nucleus starts with 'but'.\"),\n",
    "        \"SN_CONTAINS_BECAUSE\": \n",
    "            defs.Reason(\n",
    "                5, \n",
    "                \"Satellite's nucleus contains 'because'.\"\n",
    "            ),\n",
    "        \"SN_STARTS_WITH_IT_MAKES\":\n",
    "            defs.Reason(\n",
    "                6,\n",
    "                \"Satellite's nucleus starts with 'It makes/made'.\"\n",
    "            )\n",
    "    }\n",
    "    \n",
    "    def generate_statement(self, text, relation, verbose=False):\n",
    "        assert(relation is not None and relation.type == \"Explanation\")\n",
    "        info = preparation.Preprocessor.prepare_extended_info(\n",
    "            text, relation, verbose\n",
    "        ) # finding N,S and the nested ones\n",
    "        if info is None:\n",
    "            utils.print_if_verbose(\"Extended info preparation wasn't successful.\", verbose)\n",
    "            return None\n",
    "\n",
    "        nucleus_tokens = utils.lowercase_and_tokenize(\n",
    "            info.nucleus_preparation_result.prepared_text\n",
    "        )\n",
    "        assert len(nucleus_tokens) > 0\n",
    "        utils.print_if_verbose(\n",
    "            \"Satellite's nucleus text: \\n\"\n",
    "            f\"{info.satellite_preparation_result.prepared_text}\", \n",
    "            verbose\n",
    "        )\n",
    "        sn_tokens = utils.lowercase_and_tokenize(\n",
    "            info.satellite_preparation_result.prepared_text\n",
    "        )\n",
    "        assert len(sn_tokens) > 0\n",
    "\n",
    "        reason = None\n",
    "        augmented_sn_text = info.satellite_preparation_result.prepared_text\n",
    "\n",
    "        if nucleus_tokens[0] == \"but\":\n",
    "            reason = RuleExplanation01.reasons[\"N_STARTS_WITH_BUT\"]\n",
    "        elif (\n",
    "            nucleus_tokens[0] == \"if\"\n",
    "                and (\n",
    "                    info.nucleus_info.relation is None\n",
    "                        or utils.get_relation_type(\n",
    "                            info.nucleus_info.relation.left_child\n",
    "                        ) != \"Condition\"\n",
    "                )\n",
    "        ):\n",
    "            reason = RuleExplanation01.reasons[\"N_STARTS_WITH_IF\"]       \n",
    "        elif \"because\" in nucleus_tokens:\n",
    "            reason = RuleExplanation01.reasons[\"N_CONTAINS_BECAUSE\"]\n",
    "        elif sn_tokens[0] == 'but':\n",
    "            reason = RuleExplanation01.reasons[\"SN_STARTS_WITH_BUT\"]\n",
    "        elif \"because\" in set(sn_tokens):\n",
    "            reason = RuleExplanation01.reasons[\"SN_CONTAINS_BECAUSE\"]\n",
    "        else:\n",
    "            satellite_text = preparation.clean(\n",
    "                text[\n",
    "                    info.satellite_info.segment.start:info.satellite_info.segment.end\n",
    "                ]\n",
    "            )\n",
    "            utils.print_if_verbose(f\"Satellite:\\n{satellite_text}\", verbose)\n",
    "            it_make_parse_result = parse_it_makes(word_tokenize(satellite_text))\n",
    "            utils.print_if_verbose(it_make_parse_result, verbose)\n",
    "            if it_make_parse_result.success:\n",
    "                reason = RuleExplanation01.reasons[\"SN_STARTS_WITH_IT_MAKES\"]\n",
    "                assert it_make_parse_result.subject is not None\n",
    "                assert it_make_parse_result.verb is not None\n",
    "                assert it_make_parse_result.rest is not None\n",
    "                augmented_sn_text = preparation.clean(\n",
    "                        \" \".join(\n",
    "                        [\n",
    "                            it_make_parse_result.subject, \n",
    "                            it_make_parse_result.verb, \n",
    "                            it_make_parse_result.rest\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if reason is not None:\n",
    "            utils.print_if_verbose(reason.explanation, verbose)\n",
    "            \n",
    "            prepared_nucleus_text = utils.remove_trailing_punctuation(\n",
    "                utils.uppercase_first_letter(info.nucleus_preparation_result.prepared_text)\n",
    "            )\n",
    "            processed_sn_text = nlp.remove_leading_words(augmented_sn_text, verbose)\n",
    "            prepared_sn_text = utils.lowercase_first_letter(\n",
    "                processed_sn_text if processed_sn_text is not None \n",
    "                    else info.sn_text\n",
    "            )\n",
    "            \n",
    "            statement_text = f\"{prepared_nucleus_text} because {prepared_sn_text}\"\n",
    "            return defs.Statement(\n",
    "                statement_text=statement_text,\n",
    "                nucleus=prepared_nucleus_text,\n",
    "                satellite_nucleus=prepared_sn_text,\n",
    "                left_boundary=relation.left.start,\n",
    "                right_boundary=relation.right.end,\n",
    "                nucleus_proximity=info.nucleus_proximity.value,\n",
    "                rule=self.name,\n",
    "                reason=reason\n",
    "            )\n",
    "        else:\n",
    "            utils.print_if_verbose(\"None of the conditions were met.\", verbose)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 'm often there for three hours .  It makes me feel great to do something for the environment .  \n",
      "Nucleus is on the left.\n",
      "Nucleus's depth <= 3.\n",
      "Nucleus is flat.\n",
      "Will use the whole segment.\n",
      "Satellite's nucleus is on the right.\n",
      "Nuclei proximity is NucleusProximity.FAR\n",
      "The depth of the satellite's nucleus <= 3.\n",
      "Satellite's left subsegment is not nucleus.\n",
      "Will use the whole segment.\n",
      "Satellite's nucleus text: \n",
      "It makes me feel great to do something for the environment.\n",
      "Satellite:\n",
      "It makes me feel great to do something for the environment.\n",
      "ItMakesParseResult(success=True, subject='I', verb='feel', rest='great to do something for the environment .')\n",
      "Satellite's nucleus starts with 'It makes/made'.\n",
      "Removing tokens before the first NP:\n",
      "-- syntactic parsing result\n",
      " (ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP (VBP feel)\n",
      "      (ADJP (JJ great)\n",
      "        (S\n",
      "          (VP (TO to)\n",
      "            (VP (VB do)\n",
      "              (NP (NN something))\n",
      "              (PP (IN for)\n",
      "                (NP (DT the) (NN environment))))))))\n",
      "    (. .)))\n",
      "I feel great to do something for the environment. \n",
      "---> \n",
      "I feel great to do something for the environment.\n",
      "\n",
      "RESULT:\n",
      "{\n",
      "  \"statement_text\": \"I'm often there for three hours because I feel great to do something for the environment.\",\n",
      "  \"nucleus\": \"I'm often there for three hours\",\n",
      "  \"satellite_nucleus\": \"I feel great to do something for the environment.\",\n",
      "  \"left_boundary\": 1007,\n",
      "  \"right_boundary\": 1105,\n",
      "  \"nucleus_proximity\": \"far\",\n",
      "  \"rule\": \"explanation_01\",\n",
      "  \"reason\": [\n",
      "    6,\n",
      "    \"Satellite's nucleus starts with 'It makes/made'.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and \"__file__\" not in globals():\n",
    "    rule = RuleExplanation01()\n",
    "\n",
    "    with open(\"../parsed/race/train/middle/8123.txt.tree\", \"rt\") as f:\n",
    "        tree_text = f.read()\n",
    "\n",
    "    text, relations = relation_extraction.read_relations(\n",
    "        tree_text.replace(\"<s>\", \"\").replace(\"<P>\", \"\")\n",
    "    )\n",
    "\n",
    "    expl = relations[\"Explanation\"][1] # for generating just 1 statement\n",
    "    print(text[expl.left.start:expl.right.end])\n",
    "\n",
    "    statement = rule.generate_statement(text, expl, verbose=True)\n",
    "    print(\"\\nRESULT:\")\n",
    "    print(json.dumps(statement._asdict(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
