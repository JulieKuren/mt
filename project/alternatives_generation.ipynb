{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import enum\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /Users/YK/mt/project/aux/defs.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/relation_extraction.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/utils.ipynb\n",
      "importing Jupyter notebook from /Users/YK/mt/project/aux/nlp.ipynb\n",
      "importing Jupyter notebook from preparation.ipynb\n",
      "importing Jupyter notebook from rule_base.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import aux.defs\n",
    "import aux.relation_extraction\n",
    "import aux.utils\n",
    "import aux.nlp\n",
    "import preparation\n",
    "\n",
    "%run explanation_04.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATEMENTS_DIR = \"/Users/YK/mt/project/statements_5/\"\n",
    "RACE_PART = \"test/middle\"\n",
    "RACE_DIR = \"/Users/YK/mt/RACE\"\n",
    "PARSED_RACE_DIR = \"/Users/YK/mt/parsed/race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Position(enum.Enum):\n",
    "    BEFORE = \"before\"\n",
    "    NESTED = \"nested\"\n",
    "    AFTER  = \"after\"\n",
    "\n",
    "    \n",
    "Alternative = collections.namedtuple(\n",
    "    \"Alternative\",\n",
    "    [\n",
    "        \"true_statement\",\n",
    "        \"nuclei_hash\",\n",
    "        \"alternative_statement\",\n",
    "        \"relation_type\",\n",
    "        \"position\", \n",
    "        \"distance_words\",\n",
    "        \"distance_sentences\",\n",
    "        \"sn_length\",\n",
    "        \"sn_length_relative_difference\",\n",
    "        \"jaccard_distance\",\n",
    "        \"edit_distance\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_statements(directory, subdirectories, text_no):\n",
    "    statements = {}\n",
    "    for subdirectory in subdirectories:\n",
    "        file_path = os.path.join(\n",
    "            directory, subdirectory, RACE_PART, f\"{text_no}.txt.tree\"\n",
    "        )\n",
    "        if os.path.exists(file_path): \n",
    "            with open(file_path, \"rt\") as f:\n",
    "                statements[subdirectory] = json.load(f)\n",
    "    return statements\n",
    "    \n",
    "\n",
    "def load_relations(text_no, directory):\n",
    "    file_path = os.path.join(directory, f\"{text_no}.txt.tree\")\n",
    "    if os.path.exists(file_path):\n",
    "        text, relations, _ = aux.relation_extraction.load_relations(\n",
    "            os.path.join(directory, f\"{text_no}.txt.tree\")\n",
    "        )\n",
    "        return text, relations # {t: relations[t] for t in types if t in relations}\n",
    "    else:\n",
    "        return \"\", {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statements_subdirectories = [\n",
    "    f for f in os.listdir(STATEMENTS_DIR) if os.path.isdir(os.path.join(STATEMENTS_DIR, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_words(text_span):\n",
    "    return len(nltk.tokenize.word_tokenize(text_span))\n",
    "\n",
    "\n",
    "def get_n_sentences(text_span):\n",
    "    cnt = 0\n",
    "    for c in text_span:\n",
    "        if c in {'.', ',', '!'}:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def get_position_and_distance(statement, relation, text, verbose=False):\n",
    "    if relation.right.end <= statement[\"left_boundary\"]:\n",
    "        span = text[relation.right.end:statement[\"left_boundary\"]]\n",
    "        return (\n",
    "            Position.BEFORE,\n",
    "            get_n_words(span),\n",
    "            get_n_sentences(span)\n",
    "        )\n",
    "    elif relation.left.start >= statement[\"right_boundary\"]:\n",
    "        span = text[statement[\"right_boundary\"]:relation.left.start]\n",
    "        return (\n",
    "            Position.AFTER,\n",
    "            get_n_words(span),\n",
    "            get_n_sentences(span)\n",
    "        )\n",
    "    else:\n",
    "        if (\n",
    "            relation.left.start < statement[\"split_point\"]\n",
    "                and relation.right.end > statement[\"split_point\"]\n",
    "        ):\n",
    "            if verbose:\n",
    "                print(\"The relation overlaps with the relation of the true statement.\")\n",
    "            return None, None, None\n",
    "        else:\n",
    "            if relation.right.end <= statement[\"split_point\"]:\n",
    "                span = text[relation.right.end:statement[\"split_point\"]]\n",
    "                return Position.NESTED, get_n_words(span), get_n_sentences(span)\n",
    "            else:\n",
    "                span = (text[statement[\"split_point\"]:relation.left.start])\n",
    "                return Position.NESTED, get_n_words(span), get_n_sentences(span)\n",
    "\n",
    "    \n",
    "def get_jaccard_distance(phrase_1, phrase_2):\n",
    "    tokens_1 = set(nltk.tokenize.word_tokenize(phrase_1))\n",
    "    tokens_2 = set(nltk.tokenize.word_tokenize(phrase_2))\n",
    "    return nltk.jaccard_distance(tokens_1, tokens_2)\n",
    "\n",
    "\n",
    "def get_edit_distance(phrase_1, phrase_2):\n",
    "    return nltk.edit_distance(phrase_1, phrase_2)\n",
    "    \n",
    "\n",
    "RelationData = collections.namedtuple(\n",
    "    \"RelationData\",\n",
    "    [\"relation\", \"position\", \"distance_words\", \"distance_sentences\"]\n",
    ")\n",
    "\n",
    "\n",
    "def get_k(relation_data_list, closest, k):\n",
    "    sorted_relation_data_list = sorted(\n",
    "        relation_data_list, key=lambda rd: rd.distance_words\n",
    "    )\n",
    "    if closest:\n",
    "        return sorted_relation_data_list[:k]\n",
    "    else:\n",
    "        return sorted_relation_data_list[-k:]\n",
    "        \n",
    "        \n",
    "def filter_relations(statement, relations, text, k=2):\n",
    "    relation_data_lists = collections.defaultdict(list)\n",
    "    for relation in relations:\n",
    "        position, distance_words, distance_sentences = get_position_and_distance(\n",
    "            statement, relation, text\n",
    "        )\n",
    "        if position is not None:\n",
    "            relation_data_lists[position].append(\n",
    "                RelationData(\n",
    "                    relation=relation,\n",
    "                    position=position,\n",
    "                    distance_words=distance_words,\n",
    "                    distance_sentences=distance_sentences\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    result = []\n",
    "    result += get_k(relation_data_lists[Position.BEFORE], closest=True, k=k)\n",
    "    result += get_k(relation_data_lists[Position.AFTER], closest=True, k=k)\n",
    "    result += get_k(relation_data_lists[Position.NESTED], closest=False, k=k)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def compute_hash(string):\n",
    "    return hashlib.md5(string.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def create_alternative(statement, relation_data, true_sn_text_len, text, verbose=False):\n",
    "    relation_info = preparation.get_info(relation_data.relation, verbose)\n",
    "    assert relation_info is not None\n",
    "    if relation_info.satellite_info.relation is None:\n",
    "        if verbose:\n",
    "            print(\"Satellite is flat.\")\n",
    "        return None\n",
    "\n",
    "    satellite_handling_result = preparation.Preprocessor.handle_satellite(\n",
    "        text, relation_info.satellite_info, relation_info.nucleus_info.direction, verbose\n",
    "    )\n",
    "    if satellite_handling_result is None:\n",
    "        if verbose:\n",
    "            print(\"Satellite preprocessing was unsuccessful.\")\n",
    "        return None\n",
    "    \n",
    "    prepared_sn_text = aux.nlp.take_first_sentence_and_remove_leading_words(\n",
    "        satellite_handling_result.preparation_result.prepared_text, \n",
    "        lowercase_first_letter=True, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    if prepared_sn_text is None:\n",
    "        return None\n",
    "    sn_text_len = get_n_words(prepared_sn_text)\n",
    "    \n",
    "    true_statement_nucleus = statement[\"nucleus\"]\n",
    "    connective = statement[\"connective\"]\n",
    "    alternative_text = f\"{true_statement_nucleus}{connective}{prepared_sn_text}\"\n",
    "    return Alternative(\n",
    "        true_statement=statement[\"statement_text\"],\n",
    "        nuclei_hash=compute_hash(statement[\"nucleus\"] + statement[\"satellite_nucleus\"]),\n",
    "        alternative_statement=alternative_text,\n",
    "        position=relation_data.position.value,\n",
    "        relation_type=relation_data.relation.type,\n",
    "        distance_words=relation_data.distance_words,\n",
    "        distance_sentences=relation_data.distance_sentences,\n",
    "        sn_length=sn_text_len,\n",
    "        sn_length_relative_difference=(sn_text_len / true_sn_text_len - 1),\n",
    "        jaccard_distance=get_jaccard_distance(\n",
    "            statement[\"satellite_nucleus\"], prepared_sn_text\n",
    "        ),\n",
    "        edit_distance=get_edit_distance(\n",
    "            statement[\"satellite_nucleus\"], prepared_sn_text\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_alternatives(text_no):\n",
    "    rows = []\n",
    "    \n",
    "    text, relation_map = load_relations(\n",
    "        text_no, os.path.join(PARSED_RACE_DIR, RACE_PART)\n",
    "    )\n",
    "    if len(relation_map) > 0:\n",
    "        relations = [\n",
    "            relation for _, relations in relation_map.items() for relation in relations\n",
    "        ]\n",
    "        statement_map = load_statements(STATEMENTS_DIR, statements_subdirectories, text_no)\n",
    "        for _, statements in statement_map.items():\n",
    "            for statement in statements:\n",
    "                true_sn_text_len = get_n_words(statement[\"satellite_nucleus\"])\n",
    "                filtered_relation_data = filter_relations(statement, relations, text)\n",
    "                for relation_data in filtered_relation_data:\n",
    "                    alternative = create_alternative(\n",
    "                        statement, relation_data, true_sn_text_len, text\n",
    "                    )\n",
    "                    if alternative is not None:\n",
    "                        row_dict = alternative._asdict()\n",
    "                        row_dict.update(\n",
    "                            {\n",
    "                                \"text_no\": text_no,\n",
    "                                \"rule\": statement[\"rule\"],\n",
    "                                \"reason\": statement[\"reason\"][1]\n",
    "                            }\n",
    "                        )\n",
    "                        rows.append(\n",
    "                            row_dict\n",
    "                        )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def create_df(rows):\n",
    "    if len(rows) > 0:\n",
    "        result_df = pd.DataFrame(rows)[\n",
    "            [\n",
    "                \"text_no\",\n",
    "                \"true_statement\",\n",
    "                \"nuclei_hash\",\n",
    "                \"alternative_statement\",\n",
    "                \"relation_type\",\n",
    "                \"position\",        \n",
    "                \"distance_words\",\n",
    "                \"distance_sentences\",\n",
    "                \"sn_length\",\n",
    "                \"sn_length_relative_difference\",\n",
    "                \"jaccard_distance\",\n",
    "                \"edit_distance\",\n",
    "                \"rule\",\n",
    "                \"reason\"\n",
    "            ]\n",
    "        ]\n",
    "        result_df[\"d\"] = (\n",
    "            result_df.distance_words \n",
    "                * (1 - 2 * (result_df.position == Position.NESTED).astype(int))\n",
    "        )\n",
    "        result_df.sort_values(\n",
    "            [\"text_no\", \"rule\", \"true_statement\", \"position\", \"d\"], inplace=True\n",
    "        )\n",
    "        result_df.drop(\"d\", 1, inplace=True)\n",
    "\n",
    "        return result_df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_numbers = [\n",
    "    int(fn.split('.')[0]) for fn in os.listdir(os.path.join(RACE_DIR, RACE_PART))\n",
    "        if fn[-4:] == \".txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/362, text no. 7344]\n",
      "[   2/362, text no. 1047]\n",
      "[   3/362, text no. 6700]\n",
      "[   4/362, text no. 6927]\n",
      "[   5/362, text no.  504]\n",
      "[   6/362, text no. 8275]\n",
      "[   7/362, text no. 3485]\n",
      "[   8/362, text no.  869]\n",
      "[   9/362, text no. 2638]\n",
      "[  10/362, text no. 3297]\n",
      "[  11/362, text no. 1127]\n",
      "[  12/362, text no. 3056]\n",
      "[  13/362, text no. 5433]\n",
      "[  14/362, text no. 1133]\n",
      "[  15/362, text no. 6853]\n",
      "[  16/362, text no. 1696]\n",
      "[  17/362, text no. 2375]\n",
      "[  18/362, text no. 8102]\n",
      "[  19/362, text no. 5195]\n",
      "[  20/362, text no. 2163]\n",
      "[  21/362, text no. 2177]\n",
      "[  22/362, text no. 1522]\n",
      "[  23/362, text no.  288]\n",
      "[  24/362, text no. 2229]\n",
      "[  25/362, text no. 6703]\n",
      "[  26/362, text no. 2565]\n",
      "[  27/362, text no. 3684]\n",
      "[  28/362, text no. 1246]\n",
      "[  29/362, text no. 3323]\n",
      "[  30/362, text no. 6461]\n",
      "[  31/362, text no. 7958]\n",
      "[  32/362, text no. 5430]\n",
      "[  33/362, text no. 1124]\n",
      "[  34/362, text no. 3096]\n",
      "[  35/362, text no. 3901]\n",
      "[  36/362, text no. 6879]\n",
      "[  37/362, text no.  499]\n",
      "[  38/362, text no. 8101]\n",
      "[  39/362, text no. 1643]\n",
      "[  40/362, text no. 1125]\n",
      "[  41/362, text no. 3256]\n",
      "[  42/362, text no. 5627]\n",
      "[  43/362, text no. 3524]\n",
      "[  44/362, text no. 5960]\n",
      "[  45/362, text no. 7622]\n",
      "[  46/362, text no. 5579]\n",
      "[  47/362, text no. 1737]\n",
      "[  48/362, text no. 1051]\n",
      "[  49/362, text no. 2776]\n",
      "[  50/362, text no.  700]\n",
      "[  51/362, text no. 4515]\n",
      "[  52/362, text no. 1451]\n",
      "[  53/362, text no. 7975]\n",
      "[  54/362, text no. 5409]\n",
      "[  55/362, text no. 7578]\n",
      "[  56/362, text no. 3722]\n",
      "[  57/362, text no. 6854]\n",
      "[  58/362, text no. 4925]\n",
      "[  59/362, text no. 6673]\n",
      "[  60/362, text no. 6115]\n",
      "[  61/362, text no. 6667]\n",
      "[  62/362, text no. 4299]\n",
      "[  63/362, text no. 3521]\n",
      "[  64/362, text no. 1478]\n",
      "[  65/362, text no. 1295]\n",
      "[  66/362, text no. 2788]\n",
      "[  67/362, text no. 5965]\n",
      "[  68/362, text no.  926]\n",
      "[  69/362, text no. 1901]\n",
      "[  70/362, text no. 1726]\n",
      "[  71/362, text no. 6049]\n",
      "[  72/362, text no. 4890]\n",
      "[  73/362, text no. 3127]\n",
      "[  74/362, text no. 8066]\n",
      "[  75/362, text no. 1724]\n",
      "[  76/362, text no. 6088]\n",
      "[  77/362, text no. 2761]\n",
      "[  78/362, text no. 3325]\n",
      "[  79/362, text no. 7023]\n",
      "[  80/362, text no.  139]\n",
      "[  81/362, text no.  111]\n",
      "[  82/362, text no. 1678]\n",
      "[  83/362, text no. 5436]\n",
      "[  84/362, text no. 3046]\n",
      "[  85/362, text no. 3287]\n",
      "[  86/362, text no. 6472]\n",
      "[  87/362, text no. 3481]\n",
      "[  88/362, text no. 5580]\n",
      "[  89/362, text no. 7657]\n",
      "[  90/362, text no. 1568]\n",
      "[  91/362, text no. 1597]\n",
      "[  92/362, text no. 5901]\n",
      "[  93/362, text no. 3545]\n",
      "[  94/362, text no. 4570]\n",
      "[  95/362, text no. 2840]\n",
      "[  96/362, text no. 4000]\n",
      "[  97/362, text no. 1193]\n",
      "[  98/362, text no. 3961]\n",
      "[  99/362, text no.  348]\n",
      "[ 100/362, text no. 2302]\n",
      "[ 101/362, text no. 2869]\n",
      "[ 102/362, text no. 6366]\n",
      "[ 103/362, text no. 2666]\n",
      "[ 104/362, text no. 4203]\n",
      "[ 105/362, text no. 3222]\n",
      "[ 106/362, text no. 2896]\n",
      "[ 107/362, text no. 7859]\n",
      "[ 108/362, text no. 6206]\n",
      "[ 109/362, text no. 1555]\n",
      "[ 110/362, text no. 1964]\n",
      "[ 111/362, text no. 6979]\n",
      "[ 112/362, text no. 7468]\n",
      "[ 113/362, text no. 1019]\n",
      "[ 114/362, text no. 2512]\n",
      "[ 115/362, text no. 5269]\n",
      "[ 116/362, text no. 4188]\n",
      "[ 117/362, text no. 1972]\n",
      "[ 118/362, text no. 3340]\n",
      "[ 119/362, text no. 4349]\n",
      "[ 120/362, text no. 7668]\n",
      "[ 121/362, text no. 3368]\n",
      "[ 122/362, text no. 6370]\n",
      "[ 123/362, text no. 7085]\n",
      "[ 124/362, text no. 7907]\n",
      "[ 125/362, text no. 5862]\n",
      "[ 126/362, text no. 1609]\n",
      "[ 127/362, text no. 5490]\n",
      "[ 128/362, text no. 7287]\n",
      "[ 129/362, text no. 6199]\n",
      "[ 130/362, text no. 7293]\n",
      "[ 131/362, text no. 4942]\n",
      "[ 132/362, text no.  411]\n",
      "[ 133/362, text no. 7292]\n",
      "[ 134/362, text no.  149]\n",
      "[ 135/362, text no. 4599]\n",
      "[ 136/362, text no. 1393]\n",
      "[ 137/362, text no. 1378]\n",
      "[ 138/362, text no. 2936]\n",
      "[ 139/362, text no. 4360]\n",
      "[ 140/362, text no. 1556]\n",
      "[ 141/362, text no. 6946]\n",
      "[ 142/362, text no. 3619]\n",
      "[ 143/362, text no. 8016]\n",
      "[ 144/362, text no. 5532]\n",
      "[ 145/362, text no. 6765]\n",
      "[ 146/362, text no.  549]\n",
      "[ 147/362, text no. 4827]\n",
      "[ 148/362, text no. 3379]\n",
      "[ 149/362, text no. 1234]\n",
      "[ 150/362, text no. 5668]\n",
      "[ 151/362, text no. 3219]\n",
      "[ 152/362, text no.   70]\n",
      "[ 153/362, text no. 5318]\n",
      "[ 154/362, text no. 7527]\n",
      "[ 155/362, text no. 5495]\n",
      "[ 156/362, text no. 4953]\n",
      "[ 157/362, text no. 3797]\n",
      "[ 158/362, text no.   71]\n",
      "[ 159/362, text no. 5912]\n",
      "[ 160/362, text no. 7122]\n",
      "[ 161/362, text no. 3436]\n",
      "[ 162/362, text no. 4359]\n",
      "[ 163/362, text no.  206]\n",
      "[ 164/362, text no.  548]\n",
      "[ 165/362, text no. 2299]\n",
      "[ 166/362, text no. 5284]\n",
      "[ 167/362, text no. 5723]\n",
      "[ 168/362, text no. 5092]\n",
      "[ 169/362, text no.  774]\n",
      "[ 170/362, text no. 4575]\n",
      "[ 171/362, text no. 3540]\n",
      "[ 172/362, text no.    9]\n",
      "[ 173/362, text no. 3226]\n",
      "[ 174/362, text no. 5870]\n",
      "[ 175/362, text no. 7083]\n",
      "[ 176/362, text no. 4987]\n",
      "[ 177/362, text no. 6809]\n",
      "[ 178/362, text no. 1183]\n",
      "[ 179/362, text no.  359]\n",
      "[ 180/362, text no. 5695]\n",
      "[ 181/362, text no.  167]\n",
      "[ 182/362, text no. 2850]\n",
      "[ 183/362, text no. 7914]\n",
      "[ 184/362, text no. 3233]\n",
      "[ 185/362, text no. 5044]\n",
      "[ 186/362, text no. 8212]\n",
      "[ 187/362, text no. 6203]\n",
      "[ 188/362, text no. 6029]\n",
      "[ 189/362, text no. 4164]\n",
      "[ 190/362, text no. 2518]\n",
      "[ 191/362, text no. 7312]\n",
      "[ 192/362, text no. 6965]\n",
      "[ 193/362, text no. 6795]\n",
      "[ 194/362, text no. 1575]\n",
      "[ 195/362, text no. 8237]\n",
      "[ 196/362, text no.  156]\n",
      "[ 197/362, text no.  432]\n",
      "[ 198/362, text no. 8196]\n",
      "[ 199/362, text no.  340]\n",
      "[ 200/362, text no. 6804]\n",
      "[ 201/362, text no. 4034]\n",
      "[ 202/362, text no. 3997]\n",
      "[ 203/362, text no. 7273]\n",
      "[ 204/362, text no. 7924]\n",
      "[ 205/362, text no. 7918]\n",
      "[ 206/362, text no. 6435]\n",
      "[ 207/362, text no. 4395]\n",
      "[ 208/362, text no. 6794]\n",
      "[ 209/362, text no. 4197]\n",
      "[ 210/362, text no. 6769]\n",
      "[ 211/362, text no.  790]\n",
      "[ 212/362, text no. 5843]\n",
      "[ 213/362, text no. 2447]\n",
      "[ 214/362, text no.  357]\n",
      "[ 215/362, text no. 1198]\n",
      "[ 216/362, text no. 2491]\n",
      "[ 217/362, text no. 4023]\n",
      "[ 218/362, text no. 3770]\n",
      "[ 219/362, text no. 6393]\n",
      "[ 220/362, text no. 4590]\n",
      "[ 221/362, text no. 5895]\n",
      "[ 222/362, text no. 1371]\n",
      "[ 223/362, text no.  183]\n",
      "[ 224/362, text no. 1588]\n",
      "[ 225/362, text no. 1211]\n",
      "[ 226/362, text no.  785]\n",
      "[ 227/362, text no.  587]\n",
      "[ 228/362, text no. 7304]\n",
      "[ 229/362, text no. 5261]\n",
      "[ 230/362, text no. 3610]\n",
      "[ 231/362, text no. 7476]\n",
      "[ 232/362, text no. 3614]\n",
      "[ 233/362, text no. 6977]\n",
      "[ 234/362, text no. 6208]\n",
      "[ 235/362, text no. 1598]\n",
      "[ 236/362, text no. 4386]\n",
      "[ 237/362, text no. 2640]\n",
      "[ 238/362, text no. 1361]\n",
      "[ 239/362, text no.  434]\n",
      "[ 240/362, text no. 6194]\n",
      "[ 241/362, text no. 1837]\n",
      "[ 242/362, text no. 5853]\n",
      "[ 243/362, text no. 2682]\n",
      "[ 244/362, text no.  743]\n",
      "[ 245/362, text no. 4393]\n",
      "[ 246/362, text no. 2084]\n",
      "[ 247/362, text no. 5072]\n",
      "[ 248/362, text no. 8032]\n",
      "[ 249/362, text no.  582]\n",
      "[ 250/362, text no. 2523]\n",
      "[ 251/362, text no. 6753]\n",
      "[ 252/362, text no. 3818]\n",
      "[ 253/362, text no. 4811]\n",
      "[ 254/362, text no.  966]\n",
      "[ 255/362, text no. 6237]\n",
      "[ 256/362, text no. 7101]\n",
      "[ 257/362, text no. 7840]\n",
      "[ 258/362, text no. 7854]\n",
      "[ 259/362, text no.  999]\n",
      "[ 260/362, text no. 5689]\n",
      "[ 261/362, text no. 2870]\n",
      "[ 262/362, text no. 2441]\n",
      "[ 263/362, text no. 3005]\n",
      "[ 264/362, text no. 1821]\n",
      "[ 265/362, text no. 3010]\n",
      "[ 266/362, text no. 4757]\n",
      "[ 267/362, text no. 3986]\n",
      "[ 268/362, text no. 6632]\n",
      "[ 269/362, text no. 5677]\n",
      "[ 270/362, text no.    1]\n",
      "[ 271/362, text no. 4541]\n",
      "[ 272/362, text no. 4227]\n",
      "[ 273/362, text no. 7841]\n",
      "[ 274/362, text no. 3414]\n",
      "[ 275/362, text no. 7896]\n",
      "[ 276/362, text no. 6008]\n",
      "[ 277/362, text no.  581]\n",
      "[ 278/362, text no. 1029]\n",
      "[ 279/362, text no. 1728]\n",
      "[ 280/362, text no.  525]\n",
      "[ 281/362, text no. 6523]\n",
      "[ 282/362, text no. 3329]\n",
      "[ 283/362, text no. 1304]\n",
      "[ 284/362, text no. 7775]\n",
      "[ 285/362, text no. 6480]\n",
      "[ 286/362, text no. 7563]\n",
      "[ 287/362, text no. 4917]\n",
      "[ 288/362, text no. 1853]\n",
      "[ 289/362, text no. 4902]\n",
      "[ 290/362, text no. 6126]\n",
      "[ 291/362, text no. 2340]\n",
      "[ 292/362, text no. 1675]\n",
      "[ 293/362, text no. 4282]\n",
      "[ 294/362, text no. 2036]\n",
      "[ 295/362, text no. 2744]\n",
      "[ 296/362, text no. 8255]\n",
      "[ 297/362, text no. 8094]\n",
      "[ 298/362, text no. 1701]\n",
      "[ 299/362, text no. 5573]\n",
      "[ 300/362, text no. 6078]\n",
      "[ 301/362, text no. 2550]\n",
      "[ 302/362, text no. 2544]\n",
      "[ 303/362, text no. 7602]\n",
      "[ 304/362, text no. 8257]\n",
      "[ 305/362, text no.  730]\n",
      "[ 306/362, text no. 7038]\n",
      "[ 307/362, text no. 4531]\n",
      "[ 308/362, text no. 7992]\n",
      "[ 309/362, text no. 2801]\n",
      "[ 310/362, text no.  644]\n",
      "[ 311/362, text no. 7548]\n",
      "[ 312/362, text no. 3920]\n",
      "[ 313/362, text no. 3908]\n",
      "[ 314/362, text no. 7549]\n",
      "[ 315/362, text no.  719]\n",
      "[ 316/362, text no. 2790]\n",
      "[ 317/362, text no. 7159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 318/362, text no. 6535]\n",
      "[ 319/362, text no. 4322]\n",
      "[ 320/362, text no. 5772]\n",
      "[ 321/362, text no.  241]\n",
      "[ 322/362, text no. 3868]\n",
      "[ 323/362, text no. 6910]\n",
      "[ 324/362, text no. 6723]\n",
      "[ 325/362, text no. 7429]\n",
      "[ 326/362, text no. 3111]\n",
      "[ 327/362, text no.  251]\n",
      "[ 328/362, text no. 3878]\n",
      "[ 329/362, text no. 5038]\n",
      "[ 330/362, text no.  912]\n",
      "[ 331/362, text no. 2970]\n",
      "[ 332/362, text no. 1470]\n",
      "[ 333/362, text no. 2409]\n",
      "[ 334/362, text no. 5399]\n",
      "[ 335/362, text no. 3919]\n",
      "[ 336/362, text no. 3702]\n",
      "[ 337/362, text no.  867]\n",
      "[ 338/362, text no. 4535]\n",
      "[ 339/362, text no. 3474]\n",
      "[ 340/362, text no. 5005]\n",
      "[ 341/362, text no.  278]\n",
      "[ 342/362, text no. 5207]\n",
      "[ 343/362, text no. 7438]\n",
      "[ 344/362, text no. 1049]\n",
      "[ 345/362, text no. 3886]\n",
      "[ 346/362, text no. 1711]\n",
      "[ 347/362, text no.  118]\n",
      "[ 348/362, text no. 2436]\n",
      "[ 349/362, text no. 4735]\n",
      "[ 350/362, text no. 2387]\n",
      "[ 351/362, text no. 7598]\n",
      "[ 352/362, text no. 3503]\n",
      "[ 353/362, text no. 7765]\n",
      "[ 354/362, text no. 1314]\n",
      "[ 355/362, text no. 3259]\n",
      "[ 356/362, text no. 2147]\n",
      "[ 357/362, text no. 6453]\n",
      "[ 358/362, text no. 3305]\n",
      "[ 359/362, text no. 1248]\n",
      "[ 360/362, text no. 5204]\n",
      "[ 361/362, text no. 7361]\n",
      "[ 362/362, text no. 8052]\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for i, text_no in enumerate(text_numbers):\n",
    "    print(f\"[{i + 1:4d}/{len(text_numbers)}, text no. {text_no:4d}]\")\n",
    "    rows.extend(generate_alternatives(text_no))\n",
    "\n",
    "result_df = create_df(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_no</th>\n",
       "      <th>true_statement</th>\n",
       "      <th>nuclei_hash</th>\n",
       "      <th>alternative_statement</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>position</th>\n",
       "      <th>distance_words</th>\n",
       "      <th>distance_sentences</th>\n",
       "      <th>sn_length</th>\n",
       "      <th>sn_length_relative_difference</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>rule</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1</td>\n",
       "      <td>Wang has got used to it because ''I used to sp...</td>\n",
       "      <td>14d3480a7999933580fc6ff07a26a7d6</td>\n",
       "      <td>Wang has got used to it because she used to ge...</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>after</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>27</td>\n",
       "      <td>explanation_01</td>\n",
       "      <td>Nucleus starts with 'but'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>1</td>\n",
       "      <td>Wang has got used to it because ''I used to sp...</td>\n",
       "      <td>14d3480a7999933580fc6ff07a26a7d6</td>\n",
       "      <td>Wang has got used to it because ''I just looke...</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>before</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>41</td>\n",
       "      <td>explanation_01</td>\n",
       "      <td>Nucleus starts with 'but'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1</td>\n",
       "      <td>Zhang got a grade of more than 80 because ''I ...</td>\n",
       "      <td>96d4256e267d4ae7aba2f8b088cf5c7c</td>\n",
       "      <td>Zhang got a grade of more than 80 because ''I ...</td>\n",
       "      <td>Explanation</td>\n",
       "      <td>before</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>47</td>\n",
       "      <td>explanation_01</td>\n",
       "      <td>Nucleus starts with 'but'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>1</td>\n",
       "      <td>Wang has got used to it, and ''I used to speak...</td>\n",
       "      <td>14d3480a7999933580fc6ff07a26a7d6</td>\n",
       "      <td>Wang has got used to it, and she used to get a...</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>after</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>27</td>\n",
       "      <td>explanation_04</td>\n",
       "      <td>Common pattern (Whatever-Contrast).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1</td>\n",
       "      <td>Wang has got used to it, and ''I used to speak...</td>\n",
       "      <td>14d3480a7999933580fc6ff07a26a7d6</td>\n",
       "      <td>Wang has got used to it, and ''I just looked a...</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>before</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>41</td>\n",
       "      <td>explanation_04</td>\n",
       "      <td>Common pattern (Whatever-Contrast).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_no                                     true_statement  \\\n",
       "854        1  Wang has got used to it because ''I used to sp...   \n",
       "853        1  Wang has got used to it because ''I used to sp...   \n",
       "855        1  Zhang got a grade of more than 80 because ''I ...   \n",
       "852        1  Wang has got used to it, and ''I used to speak...   \n",
       "851        1  Wang has got used to it, and ''I used to speak...   \n",
       "\n",
       "                          nuclei_hash  \\\n",
       "854  14d3480a7999933580fc6ff07a26a7d6   \n",
       "853  14d3480a7999933580fc6ff07a26a7d6   \n",
       "855  96d4256e267d4ae7aba2f8b088cf5c7c   \n",
       "852  14d3480a7999933580fc6ff07a26a7d6   \n",
       "851  14d3480a7999933580fc6ff07a26a7d6   \n",
       "\n",
       "                                 alternative_statement relation_type position  \\\n",
       "854  Wang has got used to it because she used to ge...   Elaboration    after   \n",
       "853  Wang has got used to it because ''I just looke...   Elaboration   before   \n",
       "855  Zhang got a grade of more than 80 because ''I ...   Explanation   before   \n",
       "852  Wang has got used to it, and she used to get a...   Elaboration    after   \n",
       "851  Wang has got used to it, and ''I just looked a...   Elaboration   before   \n",
       "\n",
       "     distance_words  distance_sentences  sn_length  \\\n",
       "854               0                   0         10   \n",
       "853              14                   1         13   \n",
       "855              20                   4          6   \n",
       "852               0                   0         10   \n",
       "851              14                   1         13   \n",
       "\n",
       "     sn_length_relative_difference  jaccard_distance  edit_distance  \\\n",
       "854                       0.666667          0.769231             27   \n",
       "853                       1.166667          0.882353             41   \n",
       "855                      -0.538462          0.882353             47   \n",
       "852                       0.666667          0.769231             27   \n",
       "851                       1.166667          0.882353             41   \n",
       "\n",
       "               rule                               reason  \n",
       "854  explanation_01           Nucleus starts with 'but'.  \n",
       "853  explanation_01           Nucleus starts with 'but'.  \n",
       "855  explanation_01           Nucleus starts with 'but'.  \n",
       "852  explanation_04  Common pattern (Whatever-Contrast).  \n",
       "851  explanation_04  Common pattern (Whatever-Contrast).  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df.to_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"alternatives_{RACE_PART.replace('/', '-')}_{random.randint(0, 2**32):x}.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_no', 'true_statement', 'nuclei_hash', 'alternative_statement',\n",
       "       'relation_type', 'position', 'distance_words', 'distance_sentences',\n",
       "       'sn_length', 'sn_length_relative_difference', 'jaccard_distance',\n",
       "       'edit_distance', 'rule', 'reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    (result_df.jaccard_distance >= 0.3)\n",
    "        & (np.abs(result_df.sn_length_relative_difference) <= 0.5)\n",
    "        & ((result_df.position != \"nested\") | (result_df.distance_words > 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    (result_df.jaccard_distance >= 0.0)\n",
    "        & (np.abs(result_df.sn_length_relative_difference) <= 0.5)\n",
    "        & ((result_df.position != \"nested\") | (result_df.distance_words > 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_df = result_df.loc[\n",
    "    (result_df.sn_length <= 20)\n",
    "        & (result_df.jaccard_distance >= 0.3)\n",
    "        & (np.abs(result_df.sn_length_relative_difference) <= 0.5)\n",
    "        & ((result_df.position != \"nested\") | (result_df.distance_words > 0))\n",
    "]\n",
    "\n",
    "# selected_true_statements = np.random.choice(\n",
    "#     list(set(reduced_df.true_statement)), 200, replace=False\n",
    "# )\n",
    "\n",
    "# reduced_df = reduced_df.loc[\n",
    "#     reduced_df.true_statement.isin(set(selected_true_statements))\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5423580786026201"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_df) / len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_df.to_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, \n",
    "        f\"prefiltered_alternatives_{RACE_PART.replace('/', '-')}_{random.randint(0, 2**32):x}.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
