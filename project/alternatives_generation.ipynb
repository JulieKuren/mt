{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import enum\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import aux.defs\n",
    "import aux.relation_extraction\n",
    "import aux.utils\n",
    "import aux.nlp\n",
    "import preparation\n",
    "\n",
    "%run explanation_04.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATEMENTS_DIR = \"/Users/YK/mt/project/statements_2/\"\n",
    "RACE_PART = \"train/middle\"\n",
    "RACE_DIR = \"/Users/YK/mt/RACE\"\n",
    "PARSED_RACE_DIR = \"/Users/YK/mt/parsed/race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position(enum.Enum):\n",
    "    BEFORE = \"before\",\n",
    "    NESTED = \"nested\",\n",
    "    AFTER = \"after\"\n",
    "\n",
    "    \n",
    "Alternative = collections.namedtuple(\n",
    "    \"Alternative\",\n",
    "    [\n",
    "        \"true_statement\", \n",
    "        \"alternative_statement\",\n",
    "        \"position\", \n",
    "        \"distance_words\",\n",
    "        \"distance_sentences\",\n",
    "        \"sn_length\",\n",
    "        \"sn_length_relative_difference\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_statements(directory, subdirectories, text_no):\n",
    "    statements = {}\n",
    "    for subdirectory in subdirectories:\n",
    "        file_path = os.path.join(\n",
    "            directory, subdirectory, RACE_PART, f\"{text_no}.txt.tree\"\n",
    "        )\n",
    "        if os.path.exists(file_path): \n",
    "            with open(file_path, \"rt\") as f:\n",
    "                statements[subdirectory] = json.load(f)\n",
    "    return statements\n",
    "    \n",
    "\n",
    "def load_relations(text_no, directory):\n",
    "    text, relations, _ = aux.relation_extraction.load_relations(\n",
    "        os.path.join(directory, f\"{text_no}.txt.tree\")\n",
    "    )\n",
    "    return text, relations # {t: relations[t] for t in types if t in relations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_subdirectories = [\n",
    "    f for f in os.listdir(STATEMENTS_DIR) if os.path.isdir(os.path.join(STATEMENTS_DIR, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connective_map = {\n",
    "    \"explanation_01\": \" because \",\n",
    "    \"explanation_02\": \". That is why \",\n",
    "    \"explanation_03\": \". Moreover \",\n",
    "    \"explanation_05\": \" but \",\n",
    "    \"explanation_06\": \". Also, \",\n",
    "    \"explanation_07\": \" and \",\n",
    "    \"explanation_08\": \" then \",\n",
    "}\n",
    "\n",
    "\n",
    "def get_n_words(text_span):\n",
    "    return len(nltk.tokenize.word_tokenize(text_span))\n",
    "\n",
    "\n",
    "def get_n_sentences(text_span):\n",
    "    cnt = 0\n",
    "    for c in text_span:\n",
    "        if c in {'.', ',', '!'}:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def get_position_and_distance(statement, relation, text):\n",
    "    if relation.right.end <= statement[\"left_boundary\"]:\n",
    "        return (\n",
    "            Position.BEFORE,\n",
    "            get_n_words(text[relation.right.end:statement[\"left_boundary\"]]),\n",
    "            get_n_sentences(text[relation.right.end:statement[\"left_boundary\"]])\n",
    "        )\n",
    "    elif relation.left.start >= statement[\"right_boundary\"]:\n",
    "        return (\n",
    "            Position.AFTER,\n",
    "            get_n_words(text[statement[\"right_boundary\"]:relation.left.start]),\n",
    "            get_n_sentences(text[statement[\"right_boundary\"]:relation.left.start])\n",
    "        )\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "    \n",
    "def create_alternative(statement, relation, true_sn_text_len, text, verbose=False):\n",
    "    relation_info = preparation.get_info(relation, verbose)\n",
    "    assert relation_info is not None\n",
    "    if relation_info.satellite_info.relation is None:\n",
    "        if verbose:\n",
    "            print(\"Satellite is flat.\")\n",
    "        return None\n",
    "    \n",
    "    position, distance_words, distance_sentences = get_position_and_distance(\n",
    "        statement, relation, text\n",
    "    )\n",
    "    if position is None:\n",
    "        if verbose:\n",
    "            print(\"The relation is nested within the relation of the true statement.\")\n",
    "        return None\n",
    "    \n",
    "    satellite_handling_result = preparation.Preprocessor.handle_satellite(\n",
    "        text, relation_info.satellite_info, relation_info.nucleus_info.direction, verbose\n",
    "    )\n",
    "    if satellite_handling_result is None:\n",
    "        if verbose:\n",
    "            print(\"Satellite preprocessing was unsuccessful.\")\n",
    "        return None\n",
    "    \n",
    "    processed_sn_text = aux.nlp.take_first_sentence_and_remove_leading_words(\n",
    "        satellite_handling_result.preparation_result.prepared_text, verbose\n",
    "    )\n",
    "    prepared_sn_text = utils.lowercase_first_letter(\n",
    "        processed_sn_text if processed_sn_text is not None \n",
    "            else info.satellite_preparation_result.prepared_text\n",
    "    )\n",
    "    sn_text_len = get_n_words(prepared_sn_text)\n",
    "    \n",
    "    true_statement_nucleus = statement[\"nucleus\"]\n",
    "    \n",
    "    if statement[\"rule\"] == RuleExplanation04.name:\n",
    "        alternative_text = RuleExplanation04.create_statement_text(\n",
    "           true_statement_nucleus , prepared_sn_text\n",
    "        )\n",
    "    else:\n",
    "        if statement[\"rule\"] in connective_map:\n",
    "            connective = connective_map[statement[\"rule\"]]\n",
    "            alternative_text = f\"{true_statement_nucleus}{connective}{prepared_sn_text}\"\n",
    "        else:\n",
    "            alternative_text = None\n",
    "    \n",
    "    if alternative_text is None:\n",
    "        return None\n",
    "    else:\n",
    "        return Alternative(\n",
    "            true_statement=statement[\"statement_text\"],\n",
    "            alternative_statement=alternative_text,\n",
    "            position=position,\n",
    "            distance_words=distance_words,\n",
    "            distance_sentences=distance_sentences,\n",
    "            sn_length=sn_text_len,\n",
    "            sn_length_relative_difference=(sn_text_len / true_sn_text_len - 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_no = 2111\n",
    "\n",
    "statements = load_statements(STATEMENTS_DIR, statements_subdirectories, text_no)\n",
    "text, relation_map = load_relations(\n",
    "    text_no, os.path.join(PARSED_RACE_DIR, RACE_PART)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_relation = relation_map[\"Elaboration\"][8]\n",
    "tmp_statement = statements[\"explanation_01\"][0]\n",
    "true_sn_text_len = get_n_words(tmp_statement[\"satellite_nucleus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleus is on the left.\n",
      "Satellite's nucleus is on the left.\n",
      "Nuclei proximity is NucleusProximity.NEAR\n",
      "Satellite's (left) nucleus contains '.', '!', '?', or ';'.\n",
      "Will use the whole segment.\n",
      "Text extracted from the satellite:\n",
      "The Japanese called it \" kaizen \", which means \" improvement \". Maurer studied the idea and did some experiments with it.  \" Kaizen \" could possibly help people succeed in doing everything.\n",
      "Taking the first sentence and removing leading words:\n",
      "-- syntactic parsing result\n",
      " (ROOT\n",
      "  (S\n",
      "    (VP\n",
      "      (NP\n",
      "        (NP\n",
      "          (NP (DT The) (JJ Japanese))\n",
      "          (VP (VBN called)\n",
      "            (NP (PRP it) ('' ''))))\n",
      "        (PRN (FW kaizen) ('' '')\n",
      "          (S (, ,)\n",
      "            (NP (WDT which))\n",
      "            (VP (VBZ means))\n",
      "            ('' '')))\n",
      "        (NP (NN improvement) ('' ''))))\n",
      "    (. .)))\n",
      "The Japanese called it \" kaizen \", which means \" improvement \". Maurer studied the idea and did some experiments with it.  \" Kaizen \" could possibly help people succeed in doing everything. \n",
      "---> \n",
      "which means'' improvement''.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Alternative(true_statement='But as you start to write down your hopes for the new year, you think about the last year because you excitedly write down all the changes you are going to make, but by the end of January those ideas get lost in your busy life.', alternative_statement=\"But as you start to write down your hopes for the new year, you think about the last year because which means'' improvement''.\", position=<Position.AFTER: 'after'>, distance_words=81, distance_sentences=9, sn_length=6, sn_length_relative_difference=-0.7857142857142857)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_alternative(tmp_statement, tmp_relation, true_sn_text_len, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for relation_type, relations in relation_map.items():\n",
    "    for relation in relations:\n",
    "        position = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_alternatives(text_no):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_names = [\n",
    "    file_name for file_name in os.listdir(STATEMENTS_DIR) \n",
    "        if file_name[0] != '.' and os.path.isdir(os.path.join(STATEMENTS_DIR, file_name))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_statements = []\n",
    "\n",
    "for rule_name in rule_names:\n",
    "    directory = os.path.join(STATEMENTS_DIR, rule_name, RACE_PART)\n",
    "    for file_name in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_name), \"rt\") as f:\n",
    "            file_no_str = file_name.split(\".\", 1)[0]\n",
    "#             text_file = LINK_TEMPLATE.format(\n",
    "#                 link=f'file://{os.path.join(RACE_DIR, RACE_PART, file_no_str + \".txt\")}',\n",
    "#                 text=file_no_str + \".txt\"\n",
    "#             )\n",
    "#             tree_file = LINK_TEMPLATE.format(\n",
    "#                 link=f'file://{os.path.join(PARSED_RACE_DIR, RACE_PART, file_no_str + \".txt.tree\")}',\n",
    "#                 text=file_no_str + \".txt.tree\"\n",
    "#             )\n",
    "            statements = json.load(f)\n",
    "            for statement in statements:\n",
    "                statement.update(\n",
    "#                     {\"text_file\": text_file, \"tree_file\": tree_file}\n",
    "                    {\"text_number\": file_no_str}\n",
    "                )\n",
    "            all_statements.extend(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_statements)\n",
    "df[\"reason_number\"] = df.reason.apply(lambda r: r[0])\n",
    "df.reason = df.reason.apply(lambda r: r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\n",
    "    [\n",
    "        \"rule\", \n",
    "        \"reason\", \n",
    "        \"statement_text\", \n",
    "        \"nucleus\", \n",
    "        \"satellite_nucleus\", \n",
    "        \"text_number\",\n",
    "        \"nucleus_proximity\",\n",
    "        \"left_boundary\",\n",
    "        \"right_boundary\",\n",
    "        \"reason_number\"\n",
    "    ]\n",
    "].sort_values(\n",
    "    by=[\"rule\", \"reason_number\"]\n",
    ").drop(\n",
    "    \"reason_number\", 1\n",
    ").to_excel(\n",
    "    os.path.join(\n",
    "        STATEMENTS_DIR, f\"{RACE_PART.replace('/', '-')}_{random.randint(0, 2**32):x}.xlsx\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
